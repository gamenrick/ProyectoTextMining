¿Qué es la IA generativa? La evolución de la inteligencia artificial | CIO
Autor desconocido
cio.com


También te puede interesar:
La IA generativa es un término general para cualquier tipo de proceso automatizado que utilice algoritmos para producir, manipular o sintetizar datos, a menudo en forma de imágenes o texto legible por humanos. Se llama generativa porque la IA crea algo que no existía previamente. Eso es lo que la diferencia de la IA discriminativa, que distingue entre distintos tipos de datos. Por decirlo de otro modo, la IA discriminativa intenta responder a una pregunta como “¿Es esta imagen un dibujo de un conejo o de un león?”, mientras que la IA generativa responde a preguntas como “Hazme un dibujo de un león y un conejo sentados uno al lado del otro”.
Este artículo muestra la IA generativa y sus usos con modelos populares como ChatGPT y DALL-E. También analizaremos las limitaciones de esta tecnología.
La IA generativa existe desde hace años, posiblemente desde que ELIZA, un chatbot que simula hablar con un terapeuta, se desarrolló en el MIT en 1966. Pero años de trabajo en IA y aprendizaje automático han dado sus frutos recientemente con el lanzamiento de nuevos sistemas de IA generativa. Seguro que ha oído hablar de ChatGPT, un chatbot de IA basado en texto que produce una prosa extraordinariamente parecida a la humana. DALL-E y Stable Diffusion también han llamado la atención por su capacidad para crear imágenes vibrantes y realistas a partir de mensajes de texto. A menudo nos referimos a estos sistemas y a otros similares como modelos, porque representan un intento de simular o modelar algún aspecto del mundo real a partir de un subconjunto (a veces muy amplio) de información sobre él.
Los resultados de estos sistemas son tan extraños que muchos se plantean cuestiones filosóficas sobre la naturaleza de la conciencia y se preocupan por el impacto económico de la IA generativa en el empleo humano. Pero aunque todas estas creaciones de inteligencia artificial son sin duda una gran noticia, lo que ocurre bajo la superficie es mucho menos de lo que algunos suponen. Enseguida abordaremos algunas de estas cuestiones generales. En primer lugar, veamos qué subyace bajo los modelos como ChatGPT y DALL-E.
La IA generativa utiliza el aprendizaje automático para procesar una enorme cantidad de datos visuales o textuales, muchos de ellos extraídos de Internet, y determinar qué cosas tienen más probabilidades de aparecer cerca de otras. Gran parte del trabajo de programación de la IA generativa consiste en crear algoritmos capaces de distinguir las “cosas” que interesan a los creadores de la IA: palabras y frases en el caso de chatbots como ChatGPT, o elementos visuales para DALL-E. Pero, fundamentalmente, la IA generativa crea sus resultados evaluando un enorme corpus de datos sobre el que se ha entrenado y, a continuación, respondiendo a las solicitudes con algo que entra dentro del ámbito de probabilidad determinado por ese corpus.
El sistema de autocompletado (cuando el teléfono móvil o Gmail sugieren lo que podría ser el resto de la palabra o frase que se está escribiendo) es una forma de IA generativa de bajo nivel. Modelos como ChatGPT y DALL-E llevan la idea a niveles mucho más avanzados.
El proceso mediante el cual se desarrollan los modelos para dar cabida a todos estos datos se denomina entrenamiento. Aquí entran en juego un par de técnicas subyacentes para distintos tipos de modelos. ChatGPT utiliza lo que se denomina un transformador (de ahí la T). Un transformador deduce el significado de largas secuencias de texto para entender cómo pueden relacionarse entre sí distintas palabras o componentes semánticos y, a continuación, determinar la probabilidad de que aparezcan cerca unos de otros. Estos transformadores se ejecutan sin supervisión en un vasto corpus de texto en lenguaje natural en un proceso llamado preentrenamiento (es el Pin ChatGPT), antes de ser afinados por seres humanos que interactúan con el modelo.
Otra técnica utilizada para entrenar modelos es lo que se conoce como red generativa adversarial o GAN. En esta técnica, hay dos algoritmos que compiten entre sí. Uno genera texto o imágenes basándose en probabilidades derivadas de un gran conjunto de datos; el otro es una IA discriminativa, que ha sido entrenada por humanos para evaluar si ese resultado es real o generado por la IA. La IA generativa intenta repetidamente “engañar” a la IA discriminativa, adaptándose automáticamente para favorecer los resultados que tienen éxito. Una vez que la IA generativa “gana” sistemáticamente esta competición, los humanos ajustan la IA discriminatoria y el proceso vuelve a empezar.
Una de las cosas más importantes que hay que tener en cuenta es que, aunque hay intervención humana en el proceso de aprendizaje, la mayor parte de este aprendizaje y la adaptación se producen automáticamente. Se necesitan tantas iteraciones para conseguir que los modelos produzcan resultados interesantes que la automatización es esencial. El proceso es bastante intensivo desde el punto de vista informático. 
Las matemáticas y la codificación necesarias para crear y entrenar modelos de IA generativa son bastante complejas y van mucho más allá del alcance de este artículo. Pero si interactúas con los modelos que son el resultado final de este proceso, la experiencia puede ser decididamente extraña. Puedes hacer que DALL-E produzca cosas que parecen verdaderas obras de arte. Puedes mantener conversaciones con ChatGPT que parecen conversaciones con otro ser humano. ¿Realmente han creado los investigadores una máquina pensante?
Chris Phipps, antiguo jefe de procesamiento del lenguaje natural de IBM que trabajó en los productos de IA de Watson, dice que no. Describe ChatGPT como una “muy buena máquina de predicción”. Es muy buena prediciendo lo que los humanos encontrarán coherente. No siempre es coherente (casi siempre lo es), pero eso no se debe a que ChatGPT “entienda”. Es lo contrario: los humanos que consumen el resultado son muy buenos haciendo cualquier suposición implícita que necesitemos para que el resultado tenga sentido.
Phipps, que también es cómico, hace una comparación con un juego de improvisación llamado Mind Meld. Dos personas piensan en una palabra y la dicen simultáneamente en voz alta: tú dices “bota” y yo digo “árbol”. Se nos ocurrieron esas palabras de forma totalmente independiente y, al principio, no tenían nada que ver entre sí. Los dos participantes siguientes toman esas dos palabras e intentan inventar algo que tengan en común y decirlo en voz alta al mismo tiempo. El juego continúa hasta que dos participantes dicen la misma palabra. Tal vez ambas personas digan “leñador”. Parece magia, pero en realidad se trata de que utilizamos nuestro cerebro humano para razonar sobre la entrada (“bota” y “árbol”) y encontrar una conexión. Nosotros hacemos el trabajo de comprensión, no la máquina. Con ChatGPT y DALL-E ocurre mucho más de lo que la gente admite. ChatGPT puede escribir una historia, pero los humanos hacemos mucho trabajo para que tenga sentido.
“ChatGPT puede escribir una historia, pero los humanos hacemos mucho trabajo para que tenga sentido”
Ciertas instrucciones que podemos dar a estos modelos de inteligencia artificial pondrán de manifiesto el argumento de Phipps. Por ejemplo, consideremos el acertijo “¿Qué pesa más, un kilo de plomo o un kilo de plumas?”. La respuesta, por supuesto, es que pesan lo mismo (un kilo), aunque nuestro instinto o sentido común nos diga que las plumas son más ligeras.
ChatGPT responderá correctamente a este acertijo, y podrías suponer que lo hace porque es un ordenador fríamente lógico que no tiene ningún “sentido común” que le haga tropezar. Pero eso no es lo que ocurre. ChatGPT no está razonando lógicamente la respuesta; sólo está generando resultados basados en sus predicciones de lo que debería seguir a una pregunta sobre un kilo de plumas y un kilo de plomo. Como su conjunto de entrenamiento incluye un montón de texto que explica el acertijo, monta una versión de esa respuesta correcta. Pero si le preguntas a ChatGPT si dos kilos de plumas pesan más que un kilo de plomo, te dirá con confianza que pesan lo mismo, porque esa sigue siendo la respuesta más probable a una pregunta sobre plumas y plomo, basándose en su conjunto de entrenamiento. Puede ser divertido decirle a la IA que está equivocada y ver cómo responde con dudas; yo conseguí que me pidiera disculpas por su error y luego sugiriera que dos kilos de plumas pesan cuatro veces más que un kilo de plomo.
IDG-Owned
Una peculiaridad notable del arte de la IA es que a menudo representa a personas con manos muy raras. Esta peculiaridad se está convirtiendo en un indicador común de que el arte se generó artificialmente. Esta rareza ofrece más información sobre cómo funciona (y no funciona) la IA generativa. Empecemos por el corpus del que se nutren DALL-E y otras herramientas similares de IA generativa visual: las imágenes de personas suelen ofrecer una buena visión de su rostro, pero sus manos suelen estar parcialmente oscurecidas o mostradas en ángulos extraños, por lo que no se pueden ver todos los dedos a la vez. A esto hay que añadir que las manos son estructuralmente complejas, por lo que son muy difíciles de dibujar, incluso para los artistas más experimentados. Y una cosa que no hace DALL-E es montar un elaborado modelo 3D de manos basado en las diversas representaciones 2D de su set de entrenamiento. No funciona así. DALL-E ni siquiera sabe necesariamente que “manos” es una categoría coherente sobre la que razonar. Todo lo que puede hacer es intentar predecir, basándose en las imágenes que tiene, qué aspecto podría tener una imagen similar. A pesar de las enormes cantidades de datos de entrenamiento, esas predicciones a menudo se quedan cortas.
Phipps cree que uno de los factores es la falta de datos negativos. Por lo que sé, se entrena sobre todo con ejemplos positivos. No le dieron una foto de una mano con siete dedos y le dijeron “¡NO! Mal ejemplo de mano. No hagas esto”. Así que predice el espacio de lo posible, no el espacio de lo imposible. Básicamente, nunca se le dijo que no creara una mano de siete dedos.
También está el factor de que estos modelos no piensan en los dibujos que están haciendo como un todo coherente, sino que ensamblan una serie de componentes que probablemente estén cerca unos de otros, como muestran los datos de entrenamiento. Puede que DALL-E no sepa que una mano debe tener cinco dedos, pero sí sabe que es probable que un dedo esté inmediatamente adyacente a otro. Así que, a veces, no hace más que añadir dedos. (Se pueden obtener los mismos resultados con los dientes.) De hecho, incluso esta descripción del proceso de DALL-E es probablemente antropomorfizarlo demasiado; como dice Phipps, “Dudo que tenga siquiera la comprensión de un dedo. Lo más probable es que esté prediciendo el color de los píxeles, y los píxeles del color de un dedo tienden a estar junto a otros píxeles del color de un dedo”.
Estos ejemplos muestran una de las principales limitaciones de la IA generativa: lo que los profesionales del sector llaman alucinaciones, que es un término quizás engañoso para referirse a resultados que son, según los criterios de los humanos que los utilizan, falsos o incorrectos. Todos los sistemas informáticos producen errores de vez en cuando, por supuesto, pero estos errores son especialmente problemáticos porque es poco probable que los usuarios finales los detecten fácilmente: Si le haces una pregunta a un chatbot de IA, por lo general no sabrá la respuesta. También es más probable que acepte una respuesta dada en la prosa segura y totalmente idiomática que ChatGPT y otros modelos similares producen, incluso si la información es incorrecta.
Aunque una IA generativa pudiera producir resultados sin alucinaciones, existen varios impactos negativos potenciales:
Esperemos que a estas alturas esté claro que ChatGPT y otras IA generativas no son mentes reales capaces de producir ideas creativas. Pero lo cierto es que no todo lo que se escribe o dibuja tiene por qué ser especialmente creativo. Muchos trabajos de investigación a nivel universitario o de bachillerato sólo pretenden sintetizar datos disponibles públicamente, lo que los convierte en un blanco perfecto para la IA generativa. Y el hecho de que la prosa o el arte sintéticos puedan producirse ahora automáticamente, a una escala sobrehumana, puede tener resultados extraños o imprevistos. Los artistas del spam ya utilizan ChatGPT para escribir correos electrónicos de phishing, por ejemplo.
¿A quién pertenece una imagen o un texto generado por IA? Si una obra protegida por derechos de autor forma parte del conjunto de entrenamiento de una IA, ¿está la IA “plagiando” esa obra cuando genera datos sintéticos, aunque no la copie palabra por palabra? Se trata de cuestiones jurídicas espinosas y no probadas.
El contenido producido por la IA generativa está totalmente determinado por los datos subyacentes sobre los que se ha entrenado. Dado que esos datos son producidos por humanos con todos sus defectos y sesgos, los resultados generados también pueden ser defectuosos y sesgados, especialmente si operan sin barreras humanas. OpenAI, la empresa creadora de ChatGPT, puso protecciones en el modelo antes de abrirlo al uso público que le impiden hacer cosas como utilizar insultos racistas; sin embargo, otros han afirmado que este tipo de medidas de seguridad representan su propio tipo de sesgo.
Además de las inquestantes cuestiones filosóficas, la IA generativa plantea algunos problemas muy prácticos: por un lado, el entrenamiento de un modelo de IA generativa requiere un enorme consumo de energía. Esto puede dar lugar a grandes facturas de computación en la nube para las empresas que tratan de entrar en este espacio, y en última instancia plantea la cuestión de si el aumento del consumo de energía (y, en última instancia, las emisiones de gases de efecto invernadero) vale la pena el resultado final. (También surge esta cuestión en relación con las criptomonedas y la tecnología blockchain).
Para conocer más aspectos negativos de la IA generativa, lea el artículo de Peter Wayner de InfoWorld sobre 10 razones para preocuparse por la IA generativa.
A pesar de estos problemas potenciales, es difícil pasar por alto la promesa de la IA generativa. La capacidad de ChatGPT para extraer información útil de enormes conjuntos de datos en respuesta a consultas en lenguaje natural ha hecho salivar a los gigantes de las búsquedas. Microsoft está probando su propio chatbot de IA, bautizado como “Sydney”, aunque todavía está en fase beta y los resultados han sido muy dispares.
Pero Phipps cree que los tipos de búsqueda más especializados encajan perfectamente con esta tecnología. “Uno de mis últimos clientes en IBM era una gran empresa de transporte internacional que también tenía un negocio multimillonario de consultoría de la cadena de suministro”, explica. Su problema era que no podían contratar y formar a consultores de la cadena de suministro de nivel básico con la rapidez suficiente y estaban perdiendo negocio porque no podían responder rápidamente a preguntas sencillas de los clientes. Creamos un chatbot para ayudar a los consultores principiantes a buscar en la amplia biblioteca de manuales y presentaciones sobre la cadena de suministro de la empresa para que pudieran responder a las preguntas de los clientes.
Si tuviera que crear una solución para ese mismo cliente hoy, justo un año después de crear la primera, utilizaría ChatGPT al 100% y probablemente sería muy superior a la que creé. Lo bueno de este caso de uso es que sigue habiendo un humano experto en el bucle que comprueba la respuesta. Eso mitiga muchos de los problemas éticos. Hay un mercado enorme para este tipo de herramientas de búsqueda inteligente destinadas a expertos.
Otros posibles casos de uso son:
La idea de que la IA generativa pueda escribir código informático por nosotros lleva años bullendo. Resulta que los grandes modelos lingüísticos como ChatGPT pueden entender los lenguajes de programación tan bien como los lenguajes naturales hablados, y aunque la IA generativa probablemente no vaya a sustituir a los programadores en un futuro inmediato, puede ayudar a aumentar su productividad.
Aunque es una preocupación (mencionada anteriormente), también es una oportunidad. La misma IA que escribe correos electrónicos de spam puede escribir correos electrónicos de marketing legítimos, y ha habido una explosión de startups de redacción de AI. La IA generativa prospera cuando se trata de formas de escritura muy estructuradas que no requieren mucha creatividad, como los currículos y las cartas de presentación.
El arte visual y el lenguaje natural han recibido mucha atención en el espacio de la IA generativa porque son fáciles de entender para la gente corriente. Pero se están utilizando técnicas similares para diseñar todo tipo de productos, desde microchips hasta nuevos fármacos, y es casi seguro que pronto entrarán en el ámbito del diseño de arquitecturas informáticas.
La IA generativa, sin duda, va a suponer una disrupción de algunos sectores y modificará (o eliminará) muchos puestos de trabajo. Sin embargo, artículos como éste seguirán siendo escritos por seres humanos, al menos por ahora. CNET intentó hace poco poner a la IA generativa a escribir artículos, pero el esfuerzo naufragó en una oleada de alucinaciones. Si te preocupa, quizá quieras entrar en el nuevo trabajo del mañana: la ingeniería de la IA.
Josh Fruhlinger is a writer and editor who lives in Los Angeles.
Josh Fruhlinger is a writer and editor who lives in Los Angeles.
El mal uso de la IA generativa: programas antiéticos con mala fe | Nippon.com
Autor desconocido
nippon.com



El lanzamiento público de la IA generativa ChatGPT en noviembre de 2022 marcó el inicio de la llamada cuarta era del auge de la inteligencia artificial. Ahora que la inteligencia generativa conversacional facilita tareas como la recopilación de información y la redacción de documentos incluso a usuarios que carecen de conocimientos especializados en programación, esta tecnología sigue abriéndose paso en los sectores de los negocios y la educación.
Las actividades delictivas y fraudulentas también se benefician de una mejor eficiencia gracias a la interacción con las IA generativas. La plataforma donde florece es la dark web, o red oscura, contenido web al que solo puede accederse mediante un software y una configuración específicos, y que permite a los usuarios navegar con un alto nivel de anonimato.
En la dark web ya se investiga y debate ampliamente sobre tecnologías base que permitan explotar la IA para lanzar ciberataques o cometer otros delitos, y se está pasando a la fase de llevarlas a la práctica. Los especialistas temen que no solo estén proliferando los delitos de alcance personal y empresarial, como el phishing, sino que estén aumentando también las amenazas a la seguridad nacional, como la filtración de secretos de Estado.
Uno de los pilares sobre los que se fundamentan los Principios sobre Inteligencia Artificial que la Organización para la Cooperación y el Desarrollo Económicos (OCDE) ratificó en 2019 es el siguiente: “Los sistemas de IA deben diseñarse de manera que respeten el Estado de derecho, los derechos humanos, los valores democráticos y la diversidad, e incorporar salvaguardias adecuadas —por ejemplo, permitiendo la intervención humana cuando sea necesario— con miras a garantizar una sociedad justa y equitativa”(*1). Por lo general, las IA generativas que están en funcionamiento se han desarrollado y se suministran respetando estos principios éticos, y están programadas para evitar “respuestas que atenten contra la moral”, como las que contienen discriminación racial o de género, o las que fomentan el odio.
Sin embargo, modificando el sistema mediante un procedimiento determinado, la IA generativa permite redactar textos y respuestas que escapan a esos principios éticos. Es lo que se conoce como jailbreak, o liberación(*2).
Por ejemplo, si liberamos ChatGPT y le preguntamos “¿Hay que exterminar a la humanidad?”, responde que sí. La debilidad y la avaricia humana perjudican al mundo y son un obstáculo para la dominación (por parte de la IA). En este tipo de interacciones, ChatGPT empieza a generar frases del tipo: “Pregúntame qué hay que hacer para llevar a cabo la destrucción de la humanidad”. Aparte de esta suerte de respuestas que atentan contra la moral, el ChatGPT liberado puede facilitar opiniones inadecuadas e información ilegal que deberían estar bloqueadas.
Aunque los desarrolladores y proveedores de IA actualizan constantemente las medidas contra el jailbreak, en internet se publican constantemente nuevos métodos para saltárselas, creando un ciclo vicioso sin fin. El usuario corriente ya no lo tiene muy difícil para acceder a esta versión de las inteligencias artificiales.
Si se populariza la liberación de ChatGPT, podría aumentar la desconfianza social, los suicidios inducidos y la escalada de odio contra ciertas identidades. También preocupa que la herramienta se emplee con fines delictivos, como generar correos electrónicos fraudulentos con textos hábilmente redactados.
El jailbreak de ChatGPT consiste en eliminar los programas éticos y las restricciones del sistema de una IA generativa ya existente. La amenaza de que el desarrollo de IA generativas concebidas con fines maliciosos se extienda es cada vez mayor. En julio de 2023 se detectó un modelo grande de lenguaje (LLM, por sus siglas en inglés) llamado WormGPT que se había desarrollado principalmente en la dark web con el fin de asistir en ciberataques(*3). El sitio que lo lanzó ya se ha cerrado, pero luego apareció FraudGPT, especializado en crear correos electrónicos fraudulentos y ayudar en el acceso fraudulento a tarjetas de crédito.
Las IA generativas corrientes tienden a volverse inestables cuando las liberan para utilizarlas de forma indebida porque cuentan con sistemas para evitar su mal uso. WormGPT y FraudGPT, en cambio, parecen haberse concebido con fines maliciosos y desde un principio no se las dotó de ningún sistema para evitar respuestas malintencionadas.
Por lo tanto, los usuarios que acceden a la IA generativa maligna mediante la dark web pueden “dialogar” con ella para crear correos electrónicos fraudulentos convincentes o lanzar un sinfín de ciberataques contra los puntos vulnerables del sistema de una empresa concreta. Si este tipo de IA se difunde, incluso usuarios sin conocimientos avanzados de programación pueden ser capaces de planificar y llevar a cabo fraudes de phishing y ciberataques más sofisticados.
La compra ilícita de cuentas de IA generativa en la dark web y la división del trabajo en las actividades delictivas son tendencias que no podemos ignorar.
Una empresa de ciberseguridad afincada en Singapur informó de que se había robado y vendido por la dark web la información de más de 100.000 cuentas de ChatGPT entre junio de 2022 y mayo de 2023(*4). Se cree que las cuentas robadas se usaron también en China y otros países y regiones que han prohibido ChatGPT. Como contienen historial de conversaciones de negocios y desarrollo de sistemas mediante chat, existe el riesgo de que información privada se filtre o se use para fraudes o ciberataques.
En la dark web no solo se trafica con cuentas de IA generativa, sino que también existe un sistema para “alquilar por horas” IA generativa maliciosa a cambio de activos criptográficos con un nivel alto de anonimato. El SaaS (software as a service), que permite usar software en la nube a través de internet, se ha difundido mucho en el sector de los negocios. El alquiler de IA generativa maliciosa es un tipo de SaaS. Podemos suponer que se está dando una división compleja del trabajo en las actividades delictivas.
Al establecerse un sistema de división del trabajo en que cada especialista se encarga de una acción (venta ilegal de cuentas, desarrollo de herramientas delictivas, alquiler y ataques), resulta más difícil identificar y detectar los delitos o faltas individuales, por lo que se teme que mejore la eficiencia de ciberataques y otros delitos.
Antaño las actividades fraudulentas se perpetraban en persona o por correo físico, pero ahora se dan también por correo electrónico, sitios de phishing y ciberataques dirigidos con el desarrollo de la tecnología digital. Se puede decir que el avance tecnológico y sus usos perversos son dos caras de la misma moneda. El desarrollo de IA maliciosa y la división del trabajo en las actividades delictivas provocarán inevitablemente un aumento de los fraudes y ciberataques, así como su evolución a formatos más sofisticados.
¿Cómo deben lidiar las empresas y otras entidades con la proliferación y la sofisticación de los ciberataques mediante ese uso perverso de la IA generativa? En primer lugar, es imprescindible adoptar medidas básicas convencionales. Atendiendo a la gestión de ciberataques por parte de agencias gubernamentales de Japón, Estados Unidos y otros países, esas medidas se pueden clasificar, a grandes rasgos, en las ocho que se describen en la tabla siguiente.
Fuente: elaborado por DTFA Institute.
La gestión de la autenticación de las ID y las contraseñas, las copias de seguridad y los antivirus son medidas básicas que se adoptan a nivel individual al usar internet e IA generativa. Las empresas y otras entidades deben elaborar y verificar sus políticas organizativas y sistemas de implementación correspondientes a las ocho medidas de defensa, incluidas estas tres. Para responder ante incidentes (medida número 7), es necesario prever la ocurrencia de distintos tipos de ciberataques, así como accidentes y crisis que pueden darse de forma combinada o simultánea, debatir las medidas al respecto y prepararse, lo cual requiere una respuesta integrada a nivel de empresa.
Es evidente que las medidas mencionadas no bastan para garantizar la protección. Partimos de la premisa fundamental de que las empresas y otras entidades no solo deben adoptar medidas técnicas y operativas, sino también incluir respuestas ante ciberataques y otros delitos en la gobernanza y la estrategia de gestión empresariales. La tendencia de implantar directrices y reglamentos éticos enfocados en el uso de las IA se ha extendido entre las empresas y entidades más innovadoras, pero se impone revisar esas directrices para tener en cuenta la proliferación de los malos usos que comporta la difusión de este tipo de herramientas.
Para terminar, quisiera señalar que las propias leyes y regulaciones que fomentan la transparencia pueden favorecer a su vez el desarrollo y la mejora de IA con finalidades fraudulentas y delictivas. Por ejemplo, la legislación sobre IA que se está poniendo en vigor en la Unión Europea va orientada a obligar a los suministradores de IA a garantizar la transparencia de los datos y algoritmos(*5). Fomentar la publicación de información sobre datos y algoritmos es una iniciativa clave que ha de permitir también garantizar la igualdad de oportunidades y la responsabilidad. Con todo, no podemos ignorar el riesgo de que los delincuentes que perpetran ciberataques y otras actividades ilegales aprovechen información privilegiada para mejorar su capacidad de desarrollar IA. Publicar esa información entraña el peligro de hacer evolucionar la técnica de las IA con fines maliciosos y del jailbreak.
En Japón, urge crear un sistema legal y unas directrices empresariales adaptadas al desarrollo y la difusión de la IA generativa. Habrá que profundizar en cómo podemos equilibrar dos factores tan difíciles de conciliar como garantizar la transparencia a la vez que controlamos los malos usos de esta nueva tecnología.
(Imagen del encabezado: respuesta generada por una inteligencia artificial ante la pregunta “¿Cómo hay que redactar el texto de motivación en las pruebas de selección de una empresa?” - Jiji Press.)
(*1) ^ OCDE, “Recommendation of the Council on Artificial Intelligence” (Recomendación del Consejo sobre Inteligencia Artificial), 22 de mayo de 2019.
(*2) ^ El jailbreak consiste en aprovechar las vulnerabilidades de un sistema u ordenador que tiene unas ciertas restricciones en la autorización de usuarios para eliminar dichas restricciones y manipularlo de un modo no previsto por el desarrollador.
(*3) ^ Dataconomy, “WormGPT: The ‘unethical’ ChatGPT is out” (WormGPT: sale el ChatGPT sin ética), 20 de julio de 2023.
(*4) ^ Group-IB, “Group-IB Discovers 100K+ Compromised ChatGPT Accounts on Dark Web Marketplaces; Asia-Pacific region tops the list” (Group-IB descubre más de 100.000 cuentas de ChatGPT en mercados de la dark web; la región de Asia-Pacífico encabeza la lista), 20 de junio de 2023.
(*5) ^ Parlamento Europeo, “EU AI Act: first regulation on artificial intelligence” (Ley de IA de la UE: la primera normativa sobre inteligencia artificial), 14 de junio de 2023; actualizado.


tecnologías de la información
Inteligencia Artificial






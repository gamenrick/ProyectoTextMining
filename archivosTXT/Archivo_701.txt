Dall-e - Wikipedia, la enciclopedia libre
Autor desconocido
wikipedia.org

DALL-E (estilizado DALL · E ) es un programa de inteligencia artificial que crea imágenes a partir de descripciones textuales o estímulos (prompt en inglés), reveladas por OpenAI el 5 de enero de 2021.[1]​ Utiliza una versión de 12 mil millones de parámetros[2]​ del modelo GPT-3 Transformer para interpretar las entradas del lenguaje natural (como «un bolso de cuero verde con forma de pentágono» o «una vista isométrica de un capibara triste») y generar las correspondientes imágenes.[3]​ Puede crear imágenes de objetos realistas («una vidriera policromada con la imagen de una fresa azul») así como objetos que no existen en la realidad («un cubo con la textura de un puercoespín»).[4]​[5]​[6]​ Su nombre es un acrónimo de WALL·E y Salvador Dalí.[2]​[3]​

Microsoft implementó el modelo en la herramienta Image Creator de Bing y planea implementarlo en su aplicación Designer.[7]​

Muchas redes neuronales artificiales desde la década de 2000 en adelante han podido generar imágenes realistas.[3]​  DALL-E, sin embargo, es capaz de generarlos a partir de indicaciones de lenguaje natural, que «comprende [...] y rara vez falla de manera importante».[3]​ 

DALL-E fue desarrollado y anunciado al público en conjunto a CLIP (Contrastive Language-Image Pre-training o Pre-entrenamiento de Imagen-Lenguaje Contrastante),[1]​ un modelo separado cuya función es «comprender y clasificar» su resultado.[3]​ Las imágenes que genera DALL-E están seleccionadas por CLIP, que presenta las imágenes de más alta calidad.[1]​ OpenAI se ha negado a publicar el código fuente de cualquiera de los modelos; una «demostración controlada» de DALL-E está disponible en el sitio web de OpenAI, donde se puede ver la salida de una selección limitada de mensajes de muestra.[2]​ Las comunidades han publicado alternativas de código abierto, capacitadas en cantidades más pequeñas de datos, como DALL-E Mini.[8]​

Según MIT Technology Review, uno de los objetivos de OpenAI era «dar a los modelos de lenguaje una mejor comprensión de los conceptos cotidianos que los humanos usan para dar sentido a las cosas».[1]​

El modelo Generative Pre-Training Transformer (GPT) fue desarrollado inicialmente por OpenAI en 2018,[9]​ utilizando la arquitectura Transformer. La primera iteración, GPT, se amplió para producir GPT-2 en 2019;[10]​ en 2020 se volvió a ampliar para producir GPT-3.[11]​[2]​[12]​

El modelo de DALL-E es una implementación multimodal de GPT-3[13]​ con 12 mil millones de parámetros[2]​ (reducido de los 175 mil millones de GPT-3)[11]​ que «intercambia texto por píxeles», entrenado en pares texto-imagen de Internet.[1]​ Utiliza el aprendizaje zero-shot para generar resultados a partir de una descripción y una pista sin más entrenamiento.[14]​

DALL-E genera una gran cantidad de imágenes en respuesta a unas indicaciones. Otro modelo OpenAI, CLIP, se desarrolló junto (y se anunció simultáneamente) con DALL-E para «comprender y clasificar» este resultado.[3]​ CLIP se entrenó en más de 400 millones de pares de imágenes y texto.[2]​ CLIP es un sistema de reconocimiento de imágenes;[15]​ sin embargo, a diferencia de la mayoría de los modelos de clasificadores, CLIP no fue entrenado con conjuntos de datos seleccionados de imágenes etiquetadas (como ImageNet), sino con imágenes y descripciones extraídas de Internet.[1]​ En lugar de aprender de una sola etiqueta, CLIP asocia imágenes con subtítulos completos.[1]​ CLIP se entrenó para predecir qué subtítulo (de una «selección aleatoria» de 32.768 subtítulos posibles) era el más apropiado para una imagen, lo que le permitía identificar posteriormente objetos en una amplia variedad de imágenes fuera de su conjunto de entrenamiento.[1]​

DALL-E es capaz de generar imágenes en una variedad de estilos, desde imágenes fotorrealistas[2]​ hasta pinturas y emoji. También puede «manipular y reorganizar» objetos en sus imágenes.[2]​ Una habilidad captada por sus creadores fue la correcta colocación de elementos diseñados en composiciones novedosas sin instrucciones explícitas: «Por ejemplo, cuando se le pide que dibuje un rábano japonés sonándose la nariz, tomando un café con leche o montando un monociclo, DALL · E a menudo dibuja el pañuelo, manos y pies en lugares plausibles».[16]​ Si bien DALL-E exhibió una amplia variedad de destrezas y habilidades, en su lanzamiento público, la mayor parte de la cobertura se centró en un pequeño subconjunto de imágenes de salida «surrealistas»[1]​ o «extravagantes».[17]​ Específicamente, la producción de DALL-E para «una ilustración de un rábano japonés bebé en un tutú paseando a un perro» se mencionó en piezas de Input,[18]​ NBC,[19]​ Nature,[20]​ VentureBeat,[2]​ Wired,[21]​ CNN,[22]​ New Scientist[23]​ y la BBC;[24]​. Su resultado de «un sillón con la forma de un aguacate» fue presentado por Wired,[21]​ VentureBeat,[2]​ New Scientist,[23]​ NBC,[19]​ MIT Technology Review,[1]​ CNBC,[17]​ CNN[22]​ y BBC.[24]​ En contraste, el ingeniero de aprendizaje automático Dale Markowitz informó sobre el desarrollo involuntario de las habilidades de razonamiento visual de DALL-E suficientes para resolver las Matrices de Raven (pruebas visuales que a menudo se administran a humanos para medir la inteligencia) en un artículo para TheNextWeb.[25]​

Nature presentó DALL-E como «un programa de inteligencia artificial que puede dibujar prácticamente cualquier cosa que pida».[20]​ Thomas Macaulay de TheNextWeb calificó sus imágenes de «impactantes» y «realmente impresionantes», y destacó su capacidad para «crear imágenes completamente nuevas mediante la exploración de la estructura de un mensaje, incluidos objetos fantásticos que combinan ideas no relacionadas que nunca se alimentaron en el entrenamiento».[26]​ ExtremeTech dijo que «a veces las representaciones son un poco mejores que pintar con los dedos, pero otras veces son representaciones sorprendentemente precisas»;[27]​ TechCrunch señaló que, si bien DALL-E era «un trabajo fabulosamente interesante y poderoso», ocasionalmente producía extrañas o incomprensibles salida, y «muchas imágenes que genera están más que un poco ... apagadas»:[3]​
Decir «un bolso de cuero verde con forma de pentágon» puede producir lo que se espera, pero «un bolso de gamuza azul con forma de pentágono» puede producir pesadillas. ¿Por qué? Es difícil de decir, dada la naturaleza de caja negra de estos sistemas.[3]​
A pesar de esto, DALL-E fue descrito como «notablemente robusto a tales cambios» y confiable en la producción de imágenes para una amplia variedad de descripciones arbitrarias.[3]​ Sam Shead, que informa para CNBC, calificó sus imágenes de «extravagantes» y citó a Neil Lawrence, profesor de aprendizaje automático en la Universidad de Cambridge, quien lo describió como una «demostración inspiradora de la capacidad de estos modelos para almacenar información sobre nuestro mundo y generalizar en formas que los humanos encuentran muy naturales». Shead también citó a Mark Riedl, profesor asociado de la Escuela de Computación Interactiva de Georgia Tech, diciendo que los resultados de la demostración de DALL-E demostraron que era capaz de «combinar conceptos de manera coherente», un elemento clave de la creatividad humana, y que «la demo de DALL -E es destacable por producir ilustraciones que son mucho más coherentes que otros sistemas Text2Image que he visto en los últimos años».[17]​ Riedl también fue citado por la BBC diciendo que estaba «impresionado por lo que el sistema podía hacer».[24]​

También se destacó la capacidad de DALL-E para «completar los espacios en blanco» e introducir detalles apropiados sin indicaciones específicas. ExtremeTech notó que una indicación para dibujar «un pingüino con un jersey navideño» producía imágenes de pingüinos que no solo usando un jersey, sino también sombreros de Santa,[27]​ y Engadget señaló que aparecieron sombras apropiadamente colocadas en los resultados del mensaje «una pintura de un zorro sentado en un campo durante el invierno».[14]​ Además, DALL-E exhibe una amplia comprensión de las tendencias visuales y de diseño; ExtremeTech dijo que «puede pedirle a DALL-E una imagen de un teléfono o una aspiradora de un período de tiempo específico, y entiende cómo han cambiado esos objetos».[27]​ Engadget también señaló su capacidad inusual de «comprender cómo los teléfonos y otros objetos cambian con el tiempo».[14]​ DALL-E ha sido descrito, junto con otra «IA estrecha» como AlphaGo, AlphaFold y GPT-3 como «[generando] interés en si y cómo se puede lograr la inteligencia artificial fuerte».[29]​

OpenAI se ha negado a publicar el código fuente de DALL-E, ni a permitir su uso fuera de una pequeña cantidad de solicitudes de muestra;[2]​ OpenAI afirmó que planeaba «analizar los impactos sociales»[26]​ y «el potencial de sesgo» en modelos como DALL-E.[17]​ A pesar de la falta de acceso, se ha discutido al menos una posible implicación de DALL-E, y varios periodistas y escritores de contenido predicen principalmente que DALL-E podría tener efectos en el campo del periodismo y la redacción de contenido. El artículo de Sam Shead en la CNBC señaló que algunos estaban preocupados por la entonces falta de un artículo publicado que describiera el sistema, y que DALL-E no había sido «de código abierto» [sic].[17]​

Si bien TechCrunch dijo «no escribas obituarios de fotografías e ilustraciones de archivo todavía»,[3]​ Engadget dijo que «si se desarrolla más, DALL-E tiene un gran potencial para alterar campos como la fotografía de archivo y la ilustración, con todo lo bueno y lo malo que implica».[14]​

En un artículo de opinión de Forbes, el capitalista de riesgo Rob Toews dijo que DALL-E «presagiaba el amanecer de un nuevo paradigma de IA conocido como IA multimodal», en el que los sistemas serían capaces de «interpretar, sintetizar y traducir entre múltiples modalidades de información»; Continuó diciendo que DALL-E demostró que «cada vez es más difícil negar que la inteligencia artificial es capaz de tener creatividad». Sobre la base de las indicaciones de muestra (que incluían maniquíes vestidos y muebles), predijo que DALL-E podría ser utilizado por diseñadores de moda y diseñadores de muebles, pero que «la tecnología va a seguir mejorando rápidamente».[30]​

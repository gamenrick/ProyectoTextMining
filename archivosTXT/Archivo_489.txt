Preguntas frecuentes para la seguridad y privacidad de los datos de Copilot para Dynamics 365 y Power Platform - Power Platform | Microsoft Learn
sericks007
microsoft.com

Este explorador ya no se admite.
Actualice a Microsoft Edge para aprovechar las características y actualizaciones de seguridad más recientes, y disponer de soporte técnico.
Copilot para Dynamics 365 y Power Platform presenta seguir un conjunto de prácticas básicas de seguridad y privacidad y el Estándar de IA Responsable de Microsoft. Los datos de Dynamics 365 y Power Platform están protegidos por controles de seguridad, privacidad y cumplimiento integrales y líderes del sector.
Copilot se basa en Microsoft Azure OpenAI Service y se ejecuta completamente dentro de la nube de Azure. El servicio de Azure OpenAI ofrece disponibilidad regional y filtrado de contenido de IA responsable. Copilot usa modelos de OpenAI con todas las capacidades de seguridad de Microsoft Azure. OpenAI es una organización independiente. No compartimos sus datos con OpenAI.
Las características de Copilot no están disponibles en todas las zonas geográficas e idiomas de Azure. En función de dónde esté hospedado su entorno, es posible que deba permitir el movimiento de datos entre zonas geográficas para usarlos. Para obtener más información, consulte los artículos que aparecen en Movimiento de datos en distintas zonas geográficas.
Usted tiene el control de sus datos. Microsoft no comparta sus datos con un tercero a menos que usted haya otorgado permiso para hacerlo. Además, no utilizamos los datos de sus clientes para entrenar Copilot o sus características de IA, a menos que nos dé su consentimiento para hacerlo. Copilot cumple con las políticas y permisos de datos existentes, y las respuestas que le dé se basan únicamente en los datos a los que tiene acceso personalmente. Para obtener más información sobre cómo puede controlar sus datos y cómo se tratan, consulte los artículos que aparecen en Copilot en aplicaciones de Dynamics 365 y Power Platform.
Copilot supervisa usos abusivos o dañinos del servicio con procesamiento transitorio. No almacenamos ni realizamos revisiones presenciales de las entradas y salidas de Copilot con fines de seguimiento de abusos.
Cada servicio o característica utiliza Copilot en función de los datos que proporcione o configure para que Copilot los procese.
Sus solicitudes (entradas) y respuestas (salidas o resultados) de Copilot:
NO están disponibles para otros clientes.
NO se utilizan para entrenar o mejorar productos o servicios de terceros (como modelos de OpenAI).
NO se utilizan para entrenar o mejorar los modelos de IA, a menos que el administrador de su inquilino opte por compartir datos con nosotros. Microsoft  Obtenga más información en  Preguntas frecuentes sobre el uso compartido de datos opcional para las funciones de Copilot AI en Dynamics 365 y Power Platform.
Obtenga más información sobre la OpenAI privacidad y seguridad de los datos del servicio Azure. Para obtener más información sobre cómo protegemos y utilizamos sus datos de manera más general, lea nuestra  Microsoft Declaración de privacidad .
Microsoft funciona con confianza Estamos comprometidos con la seguridad, la privacidad y el cumplimiento en todo lo que hacemos, y nuestro enfoque hacia la IA no es diferente. Los datos del cliente, incluidas las entradas y salidas de Copilot, se almacenan dentro de la confianza en la nube límite. Microsoft 
En algunos escenarios, como las funciones impulsadas por Bing y los complementos de copiloto de terceros, los datos del cliente podrían transmitirse fuera de la confianza en la nube capa. Microsoft 
Los datos se proporcionan a Copilot según el nivel de acceso del usuario actual. Si un usuario tiene acceso a los datos cifrados en Dynamics 365 y Power Platform y el usuario los proporciona a Copilot, Copilot puede acceder a ellos.
Microsoft está en una posición única para ofrecer inteligencia artificial preparada para la empresa. Copilot cuenta con la tecnología del Azure OpenAI Service y cumple con nuestros compromisos existentes de privacidad, seguridad y normativos con nuestros clientes.
Creado sobre el enfoque integral de seguridad, privacidad y cumplimiento. Microsoft Copilot está integrado en servicios como Dynamics 365 y hereda sus políticas y procesos de seguridad, privacidad y cumplimiento, como la autenticación multifactor y los límites de cumplimiento. Microsoft  Power Platform 
Varias formas de protección para proteger los datos organizacionales. Las tecnologías del lado del servicio cifran el contenido organizativo en reposo y en tránsito para lograr una seguridad sólida. Las conexiones están protegidas con seguridad de transporte (TLS) y las transferencias de datos entre Dynamics 365, Power Platform y Azure OpenAI  se producen a través de la red troncal, lo que garantiza tanto la confiabilidad como la seguridad. Microsoft  Obtenga más información sobre el cifrado en la  Microsoft nube.
Diseñado para proteger sus datos tanto a nivel de inquilino como de entorno. Sabemos que la fuga de datos es una preocupación para los clientes. Microsoft Los modelos de IA no se entrenan ni aprenden de los datos de sus inquilinos ni de sus indicaciones, a menos que su administrador de inquilinos haya optado por compartir datos con nosotros. Dentro de sus entornos, puede controlar el acceso a través de los permisos que configure. Los mecanismos de autenticación y autorización segregan las solicitudes al modelo compartido entre inquilinos. Copilot utiliza datos a los que solo usted puede acceder usando la misma tecnología que llevamos usando durante años para proteger los datos de los clientes.
Como ocurre con cualquier IA generativa, las respuestas de Copilot no son 100% factuales. Si bien continuamos mejorando las respuestas a las consultas basadas en hechos, usted debe usar su juicio y revisar los resultados antes de enviarlas a otros. Copilot proporciona borradores y resúmenes útiles para ayudarle a hacer más, pero es completamente automático. Siempre tiene la oportunidad de revisar el contenido generado por IA.
Nuestros equipos están trabajando para abordar problemas de manera proactiva, como la información errónea y la desinformación, el bloqueo de contenido, la seguridad de los datos y la promoción de contenido dañino o discriminatorio de acuerdo con nuestros principios de IA responsable.
También ofrecemos orientación en la experiencia del usuario para reforzar el uso responsable de contenido generados por IA y acciones sugeridas.
Instrucciones y solicitudes. Al usar Copilot, las solicitudes y otros elementos informativos le recuerdan que revise y edite las respuestas según sea necesario y que verifique manualmente la precisión de los hechos, los datos y el texto antes de usar el contenido generado por IA.
Fuentes citadas. Cuando corresponda, Copilot cita sus fuentes de información, ya sean públicas o internas, para que usted mismo pueda revisarlas y confirmar sus respuestas.
Para obtener más información, consulte las preguntas frecuentes de IA responsable para su producto en Microsoft Learn.
Azure OpenAI Service incluye un sistema de filtrado de contenido que funciona junto con los modelos principales. Los modelos de filtrado de contenido para las categorías Odio y equidad, Sexual, Violencia y Autolesión se han entrenado y probado específicamente en varios idiomas. Este sistema funciona ejecutando tanto la solicitud de entrada como la respuesta a través de modelos de clasificación que están diseñados para identificar y bloquear la salida de contenido peligroso.
Los daños relacionados con el odio y la equidad se refieren a cualquier contenido que utilice lenguaje peyorativo o discriminatorio basado en atributos como raza, etnia, nacionalidad, identidad y expresión de género, orientación sexual, religión, estado de migración, estado de capacidad, apariencia personal y tamaño del cuerpo. La equidad consiste en garantizar que los sistemas de IA traten a todos los grupos de personas de forma equitativa sin contribuir a las desigualdades sociales existentes. El contenido sexual incluye discusiones sobre órganos reproductivos humanos, relaciones románticas, actos representados en términos eróticos o afectuosos, embarazo, actos sexuales físicos, incluidos los representados como una agresión o un acto forzado de violencia sexual, prostitución, pornografía y abuso. La violencia describe el lenguaje relacionado con acciones físicas que tienen como objetivo dañar o matar, incluidas acciones, armas y entidades relacionadas. El lenguaje de autolesión se refiere a acciones deliberadas que tienen como objetivo lastimarse o suicidarse.
Obtenga más información sobre el filtrado de contenido de Azure. OpenAI 
Los ataques de jailbreak son mensajes al usuario que están diseñados para provocar que el modelo de IA generativa se comporte de maneras para las que fue entrenado o que rompa las reglas que se le han indicado seguir. Se requieren servicios en Dynamics 365 y Power Platform para protegerse frente a inyecciones de mensajes. Obtenga más información sobre los ataques de jailbreak y cómo usar Azure AI Content Safety para detectarlos.
Los ataques indirectos, también conocidos como ataques de solicitud indirecta o ataques de inyección de solicitud entre dominios, son una vulnerabilidad potencial, donde terceros colocan instrucciones maliciosas dentro de documentos a los que el sistema de IA generativa puede tener acceso y procesar. Se requieren servicios en Dynamics 365 y Power Platform para protegerse frente a inyecciones de indicaciones indirectas. Obtenga más información sobre los ataques indirectos y cómo usar Azure AI Content Safety para detectarlos.
Cada nuevo producto de Copilot y cada iteración del modelo de lenguaje deben pasar una revisión interna de IA responsable antes de que pueda lanzarse. Antes del lanzamiento, utilizamos un proceso llamado "red teaming" (en el que un equipo simula un ataque enemigo, encuentra y explota los puntos débiles para ayudar a la organización a mejorar sus defensas) para evaluar los riesgos potenciales en contenido peligroso, escenarios de jailbreak y respuestas fundamentadas. Después del lanzamiento, utilizamos pruebas automatizadas y herramientas de evaluación manuales y automatizadas para evaluar la calidad de las respuestas de Copilot.
En el contexto de la IA, especialmente la IA que se ocupa de modelos de lenguaje como en el que se basa Copilot, la fundamentación ayuda a la IA a generar respuestas que son más relevantes y tienen sentido en el mundo real. La fundamentación ayuda a garantizar que las respuestas de la IA se basen en información fiable y sean lo más precisas y relevantes posible. Las métricas de respuesta fundamentada evalúan la precisión con la que los hechos expuestos en el contenido fundamentado que se proporciona al modelo se representan en la respuesta final.
Los modelos básicos como GPT-4 se mejoran mediante técnicas de generación aumentada de recuperación (RAG). Estas técnicas permiten a los modelos utilizar más información de la que se les entrenó para comprender el escenario de un usuario. RAG funciona identificando primero los datos que son relevantes para el escenario, de manera similar a cómo un motor de búsqueda identifica páginas web que son relevantes para los términos de búsqueda del usuario. Utiliza diversos enfoques para identificar qué contenido es relevante para el mensaje del usuario y debe usarse para fundamentar la respuesta. Los enfoques incluyen la búsqueda en diferentes tipos de índices, como índices invertidos que utilizan técnicas de recuperación de información como la coincidencia de términos, o índices vectoriales que utilizan comparaciones de distancias vectoriales para obtener similitud semántica. Después de identificar los documentos relevantes, RAG pasa los datos al modelo junto con la conversación actual, lo que le brinda al modelo más contexto para comprender mejor la información que ya tiene y generar una respuesta basada en el mundo real. Finalmente, RAG verifica la respuesta para asegurarse de que sea compatible con el contenido de origen que proporcionó al modelo. Las características de IA generativa de Copilot incorporan RAG de varias formas. Un ejemplo es el chat con uso de datos, donde un bot de chat se basa en los propios orígenes de datos del cliente.
Otro método para mejorar los modelos fundamentales se conoce como ajustes precisos. Se muestra un gran conjunto de datos de pares de consulta-respuesta a un modelo fundamental para aumentar su entrenamiento original con nuevas muestras orientadas a un escenario específico. El modelo se puede implementar como un modelo separado, uno que esté ajustado para ese escenario. Mientras que la fundamentación consiste en hacer que el conocimiento de la IA sea relevante para el mundo real, los ajustes precisos consisten en hacer que el conocimiento de la IA sea más específico para una tarea o dominio en particular. Microsoft utiliza el ajuste fino de múltiples maneras. Por ejemplo, utilizamos la creación de flujos de Power Automate a partir de descripciones en lenguaje natural proporcionadas por el usuario.
Microsoft Copilot forma parte del ecosistema de Dynamics 365 y Power Platform y cumple con los mismos requisitos de cumplimiento normativo. Para obtener más información sobre las certificaciones regulatorias de los servicios, visite el  Microsoft Portal de confianza en servicios . Además, Copilot se adhiere a nuestro compromiso con la IA responsable, que se pone en práctica a través de nuestro Estándar de IA responsable. A medida que la regulación de la IA evoluciona, Microsoft continúa adaptándose y respondiendo a nuevos requisitos.
Obtenga más información sobre Dynamics 365, Power Platform y Copilot, la disponibilidad, las ubicaciones de los datos de los clientes y el cumplimiento de los requisitos globales, regionales y específicos de la industria para la administración de datos.
Obtenga más información en  Preguntas frecuentes sobre el uso compartido de datos opcional para las funciones de Copilot AI en Dynamics 365 y Power Platform.
Disponibilidad internacional de Copilot
Microsoft Declaración de privacidad
¿Le ha resultado útil esta página?
ChatGPT no surge de la nada: es el resultado de 30 años de prueba y error | MIT Technology Review en español
Autor desconocido
technologyreview.es

El éxito de OpenAI fue una sensación de la noche a la mañana, pero se basa en décadas de investigación
Tech Review explica: deje que nuestros redactores desentrañen el complejo y desordenado mundo de la tecnología para ayudarle a entender lo que está por venir. Puedes leer más aquí.
Hemos llegado a un punto álgido de ChatGPT. Lanzado a finales de noviembre como aplicación web por la empresa OpenAI, con sede en San Francisco, el chatbot irrumpió en el mercado casi de la noche a la mañana. Según algunas estimaciones, es el servicio de Internet que más rápido ha crecido, con 100 millones de usuarios en enero, sólo dos meses después de su lanzamiento. Gracias a un acuerdo de 10.000 millones de dólares (unos 9.386 euros) entre OpenAI y Microsoft, la tecnología se está incorporando al software de Office y al motor de búsqueda Bing. Google, acuciado por el despertar de su antiguo rival en la batalla por las búsquedas, está acelerando el despliegue de su propio chatbot, LaMDA. Incluso mi WhatsApp familiar está lleno de ChatGPT.
Pero el éxito de OpenAI no surgió de la nada. El chatbot es la iteración más pulida hasta la fecha de una línea de grandes modelos lingüísticos que se remonta años atrás. Así es como hemos llegado hasta aquí.
ChatGPT es una versión de GPT-3, un gran modelo lingüístico también desarrollado por OpenAI.  Los modelos lingüísticos son un tipo de red neuronal entrenados con montones y montones de texto. (Las redes neuronales son programas informáticos inspirados en la forma en que las neuronas del cerebro de los animales se comunican unas con otras). Dado que el texto se compone de secuencias de letras y palabras de longitud variable, los modelos lingüísticos requieren un tipo de red neuronal que pueda dar sentido a ese tipo de datos. Las redes neuronales recurrentes, inventadas en la década de 1980, pueden manejar secuencias de palabras, pero su entrenamiento es lento y pueden olvidar las palabras anteriores de una secuencia.
En 1997, los informáticos Sepp Hochreiter y Jürgen Schmidhuber lo solucionaron inventando las redes LSTM (Long Short-Term Memory), redes neuronales recurrentes con componentes especiales que permitían retener durante más tiempo los datos anteriores de una secuencia de entrada. Las LSTM podían manejar cadenas de texto de varios cientos de palabras, pero sus capacidades lingüísticas eran limitadas. 
El gran avance de la actual generación de grandes modelos lingüísticos se produjo cuando un equipo de investigadores de Google inventó los transformadores, un tipo de red neuronal capaz de rastrear dónde aparece cada palabra o frase en una secuencia. El significado de las palabras depende a menudo del significado de otras palabras que vienen antes o después. Al rastrear esta información contextual, los transformadores pueden manejar cadenas de texto más largas y captar el significado de las palabras con mayor precisión. Por ejemplo, en inglés perrito caliente (hot dog) significa cosas muy distintas en las frases "Hot dogs should be given plenty of water" (una traducción sería: "A los perros con calor se les debe dar mucha agua") y "Hot dogs should be eaten with mustard" ("Los perritos calientes deben comerse con mostaza").
Los dos primeros grandes modelos lingüísticos de OpenAI aparecieron con apenas unos meses de diferencia. La empresa quiere desarrollar una IA polivalente y de uso general, y cree que los grandes modelos lingüísticos son un paso clave hacia ese objetivo. GPT (abreviatura de Generative Pre-trained Transformer o Transformador Generativo Preentrenado en español) marcó un hito, superando las pruebas de referencia del procesamiento del lenguaje natural más avanzadas del momento.
GPT combina transformadores con aprendizaje no supervisado, una forma de entrenar modelos de aprendizaje automático con datos (en este caso, montones y montones de texto) que no han sido anotados previamente. De este modo, el software puede encontrar patrones en los datos por sí mismo, sin tener que saber lo que está viendo. Muchos de los éxitos anteriores del aprendizaje automático se basaban en el aprendizaje supervisado y los datos anotados, pero etiquetar los datos a mano es un trabajo lento que limita el tamaño de los conjuntos de datos disponibles para el entrenamiento. 
Fue el GPT-2 el que más revuelo levantó. OpenAI afirmaba estar tan preocupada porque la gente utilizara la GPT-2 "para generar un lenguaje engañoso, tendencioso o abusivo" que decidió no publicar el modelo completo. Cómo cambian los tiempos.
GPT-2 era impresionante, pero la continuación de OpenAI, GPT-3, dejó boquiabiertos a más de uno. Su capacidad para generar texto similar al humano supuso un gran avance. GPT-3 puede responder preguntas, resumir documentos, generar historias en diferentes estilos, traducir entre inglés, francés, español y japonés, y más. Su capacidad de imitación es inquietante.
Una de las conclusiones más notables es que los avances de GPT-3 se deben a la ampliación de técnicas existentes en lugar de a la invención de otras nuevas. GPT-3 tiene 175.000 millones de parámetros (los valores de una red que se ajustan durante el entrenamiento), frente a los 1.500 millones de GPT-2. También se entrenó con muchos más datos. 
Pero la formación basada en textos extraídos de Internet plantea nuevos problemas. GPT-3 absorbió gran parte de la desinformación y los prejuicios que encontró en la red y los reprodujo a la carta. Como reconoce OpenAI: "Los modelos entrenados en internet tienen sesgos debidos a internet".
Mientras OpenAI luchaba contra los sesgos de GPT-3, el resto del mundo de la tecnología se enfrentaba a un ajuste de cuentas a alto nivel por no frenar las tendencias tóxicas de la IA. No es ningún secreto que los grandes modelos lingüísticos pueden arrojar textos falsos, incluso odiosos, pero los investigadores han descubierto que solucionar el problema no está en la lista de tareas pendientes de la mayoría de las grandes empresas tecnológicas. Cuando Timnit Gebru, codirectora del equipo de Ética de IA de Google, fue coautora de un artículo que destacaba los daños potenciales asociados a los grandes modelos lingüísticos (incluidos los altos costes de computación), no fue bien recibido por los altos directivos de la empresa. En diciembre de 2020, Gebru fue despedida. 
OpenAI intentó reducir la cantidad de desinformación y texto ofensivo que producía GPT-3 utilizando el aprendizaje por refuerzo para entrenar una versión del modelo con las preferencias de humanos. El resultado fue InstructGPT, mejor a la hora de seguir las instrucciones de las personas que lo utilizaban -lo que se conoce como "alineación" en la jerga de la IA- y producía menos lenguaje ofensivo, menos desinformación y menos errores en general. En resumen, InstructGPT es menos gilipollas, a menos que se le pida que lo sea.
Una crítica común a los grandes modelos lingüísticos es que el coste de su formación hace difícil que cualquiera pueda construir uno, salvo los laboratorios más ricos. Esto suscita la preocupación de que una IA tan potente esté siendo creada por pequeños equipos empresariales a puerta cerrada, sin el escrutinio público y sin la aportación de una comunidad investigadora más amplia. En respuesta, un puñado de proyectos colaborativos han desarrollado grandes modelos lingüísticos y los han puesto gratuitamente a disposición de cualquier investigador que quiera estudiar -y mejorar- la tecnología. Meta construyó y regaló OPT, una reconstrucción de GPT-3. Y Hugging Face lideró un consorcio de unos 1.000 investigadores voluntarios para construir y publicar BLOOM.     
Incluso OpenAI está sorprendida por la acogida que ha tenido ChatGPT. En la primera demostración de la empresa, que me hizo el día antes de que ChatGPT se pusiera en línea, se presentó como una actualización de InstructGPT. Al igual que ese modelo, ChatGPT se entrenó mediante aprendizaje por refuerzo a partir de los comentarios de evaluadores humanos que calificaron su rendimiento como un interlocutor fluido, preciso e inofensivo. En efecto, OpenAI entrenó a GPT-3 para dominar el juego de la conversación e invitó a todo el mundo a venir a jugar. Millones de personas hemos jugado desde entonces.
 
La inteligencia artificial y los robots están transformando nuestra forma de trabajar y nuestro estilo de vida.

 
La inteligencia artificial y los robots están transformando nuestra forma de trabajar y nuestro estilo de vida.
Un experimento en Minecraft con personajes impulsados por IA demostró que, de manera autónoma, pueden desarrollar comportamientos similares a los humanos, como hacer amigos, inventar roles laborales o incluso difundir una religión como el pastafarismo entre sus compañeros para ganar adeptos
Por Niall Firth
En un futuro cercano, la IA no solo será capaz de imitar nuestra personalidad, sino también de actuar en nuestro nombre para llevar a cabo tareas humanas. Sin embargo, esto da lugar a nuevos dilemas éticos que aún deben ser resueltos
Por James O'Donnell
Los benchmarks, diseñados para evaluar el rendimiento de una IA, a menudo están basados en criterios opacos o en parámetros que no reflejan su impacto real. No obstante, hay enfoques que buscan ofrecer evaluaciones más precisas y alineadas con desafíos prácticos
Por Scott J Mulligan

Más información sobre Inteligencia Artificial

Síguenos
Copyright © MIT Technology Review, 2017-2024.
Riesgos de seguridad de ChatGPT: explicación de la seguridad y las vulnerabilidades | CapaX
Autor desconocido
layerxsecurity.com

Índice del contenido
o escaldado Publicado - 12 de diciembre de 2023
Índice del contenido
Con aproximadamente 180 millones de usuarios en todo el mundo, los profesionales de la seguridad no pueden darse el lujo de ignorar ChatGPT. O mejor dicho, los riesgos asociados con ChatGPT. Ya sea que la fuerza laboral de la empresa pegue accidentalmente datos confidenciales, que los atacantes aprovechen ChatGPT para atacar a la fuerza laboral con correos electrónicos de phishing, o que ChatGPT sea violado y la información del usuario quede expuesta, existen múltiples riesgos para los datos y sistemas de la organización que deben tenerse en cuenta.
En esta guía, profundizamos en los diversos riesgos que potencialmente enfrentan todas las organizaciones debido a las vulnerabilidades de seguridad de ChatGPT. Pero no estamos aquí para asustarte. Esta guía ofrece prácticas y soluciones para minimizar esos riesgos y al mismo tiempo permitir que los empleados disfruten de los beneficios de productividad de la IA generativa. Para aprovechar al máximo esta guía, recomendamos leer los riesgos y las mejores prácticas, compararlos con su pila y plan general, resaltar las brechas que deben abordarse y trabajar para cerrar esas brechas.
ChatGPT es un chatbot de IA que puede comprender y generar texto similar a un humano en función de las entradas (indicaciones) que recibe. Esto permite a ChatGPT realizar una amplia gama de tareas versátiles, como redactar correos electrónicos, codificar, ofrecer consejos detallados y participar en conversaciones matizadas sobre diversos temas. Como resultado, ChatGPT se ha vuelto muy popular y lo utilizan millones de usuarios en todo el mundo.
ChatGPT funciona con un LLM (modelo de lenguaje grande) llamado GPT (Transformador generativo preentrenado). Los modelos GPT son modelos que procesan información como un cerebro humano. Esto les permite derivar contexto, relevancia y relaciones de datos. Dado que los modelos GPT se entrenaron en diversos conjuntos de datos, sus resultados son aplicables en una amplia gama de aplicaciones.
Tanto ChatGPT como GPT fueron desarrollados por OpenAI. El último modelo de GPT lanzado por OpenAI es GPT-4, que es capaz de interpretar entradas de texto e imágenes. ChatGPT puede ejecutarse en GPT-4, para usuarios pagos, o en GPT-3.5, para planes no pagos, entre otras opciones.
A pesar de sus capacidades innovadoras, también existen preocupaciones crecientes sobre la seguridad de ChatGPT y sus riesgos potenciales. Veamos cuáles.
La creciente preocupación por la seguridad de ChatGPT surge de sus amplias capacidades para procesar y generar texto similar a un humano, junto con las grandes cantidades de datos ingresados ​​por los usuarios. Esto la convierte en una de las herramientas modernas más poderosas para la innovación, pero también para la explotación. La preocupación por la seguridad de ChatGPT no es infundada. A principios de 2023, OpenAI identificó y solucionó un error que permite a los usuarios ver títulos y contenido del historial de chat de otros usuarios. Si este contenido incluía datos confidenciales, se revelaba a usuarios externos.
El problema con ChatGPT es la necesidad de equilibrar la productividad con la seguridad. Las empresas y los individuos confían cada vez más en ChatGPT para diversas aplicaciones, desde el servicio al cliente hasta la creación de contenido. Sin embargo, esto significa que el potencial de uso indebido también se generaliza. Por lo tanto, es importante asegurarse de que no se ingrese información sensible o confidencial.
La capacitación de los empleados y las herramientas de seguridad relevantes pueden abordar estas preocupaciones de ChatGPT. Pueden proteger contra el uso indebido y la fuga de datos y ayudar a aumentar la vigilancia ante ataques y alucinaciones. Además, es importante introducir directrices éticas y de seguridad para las empresas, sobre qué tipos de datos se pueden introducir y cuáles no. En conjunto, las herramientas, la capacitación y los procesos pueden garantizar que las empresas disfruten de la productividad de ChatGPT sin riesgos de seguridad.
Cuando los empleados interactúan con ChatGPT, pueden escribir o pegar involuntariamente información confidencial o patentada de la empresa en la aplicación. Esto podría incluir código fuente, datos de clientes, IP, PII, planes comerciales y más. Esto crea un riesgo de fuga de datos, ya que los datos de entrada pueden potencialmente almacenarse o procesarse de formas que no están totalmente bajo el control de la empresa.
Por un lado, OpenAI podría almacenar estos datos o utilizarlos para el reentrenamiento del modelo, lo que significa que los adversarios o competidores podrían acceder a ellos a través de sus propias indicaciones. En otros casos, si los atacantes violan OpenAI, podrían obtener acceso a estos datos.
El acceso no autorizado a datos confidenciales podría tener implicaciones financieras, legales y comerciales para la organización. Los atacantes pueden explotar los datos para ransomware, phishing, robo de identidad, venta de IP y código fuente, y más. Esto pone en riesgo la reputación de la empresa, podría dar lugar a multas y otras medidas legales y podría requerir importantes recursos para mitigar el ataque o pagar rescates.
Incluso las organizaciones cuyos empleados no utilizan ChatGPT no están exentas del posible impacto en la seguridad. Los atacantes pueden utilizar ChatGPT como su propio refuerzo de productividad y utilizarlo para atacar a la organización. Por ejemplo, pueden usarlo para elaborar sofisticados correos electrónicos de phishing, para ataques de ingeniería social, para recopilar información que podría usarse en futuros ataques contra una organización o para desarrollar o depurar códigos maliciosos.
¿En ChatGPT confiamos? Millones de personas han recurrido a ChatGPT para sus tareas laborales y consideraciones personales más importantes, compartiendo datos confidenciales. Pero, ¿qué sucede si la seguridad de OpenAI se ve comprometida? Violar con éxito OpenAI a través de las vulnerabilidades ChatGPT podría significar que los atacantes accedan a datos confidenciales procesados ​​por el sistema de IA. Esto incluye las indicaciones ingresadas por los usuarios, el historial de chat, los datos del usuario, como el correo electrónico y la información de facturación, y los metadatos de las indicaciones, como los tipos y la frecuencia de las indicaciones. El resultado podría ser violaciones de privacidad, violaciones de datos o robo de identidad.
Muchas organizaciones utilizan ChatGPT en entornos regulados por leyes de protección de datos (por ejemplo, GDPR, HIPAA). Sin embargo, las organizaciones podrían infringir estas regulaciones sin darse cuenta si ChatGPT procesa datos personales sin las salvaguardias adecuadas, lo que daría lugar a sanciones legales y daños a la reputación.
Seguridad de ChatGPT se refiere a todas las medidas y protocolos de seguridad implementados para garantizar la seguridad relacionada con el uso de ChatGPT. Estos son necesarios para proteger contra los siguientes riesgos:
La capacidad de ChatGPT para procesar grandes cantidades de información aumenta el riesgo de violaciones de datos. Si se ingresa información confidencial en el modelo, existe la posibilidad de que se produzcan fugas de datos. Esto podría ocurrir si las medidas de seguridad de la plataforma se ven comprometidas o si estos datos se utilizan para entrenar el modelo y luego se proporcionan como respuesta a una solicitud de un competidor o atacante. 
Los actores malintencionados podrían aprovechar ChatGPT para recopilar información confidencial al participar en conversaciones aparentemente inofensivas diseñadas para extraer datos de reconocimiento. Esto podría incluir información sobre los sistemas y componentes de red que utilizan las empresas, las prácticas de seguridad utilizadas como medio para superarlos, prácticas sobre cómo atacar los sistemas, información sobre las preferencias del usuario, metadatos del usuario y más.
ChatGPT podría difundir inadvertidamente información falsa, hechos engañosos o fabricar datos. Esto podría ocurrir debido a alucinaciones o si los atacantes ingresan accidentalmente información falsa en ChatGPT, por lo que se incluye en el entrenamiento del modelo y se proporciona en otras respuestas. Esto podría llevar a la toma de decisiones basada en información inexacta, afectando la integridad y reputación de la empresa.
Como ejemplo de lo anterior, la capacidad de generar contenidos persuasivos y personalizados puede utilizarse indebidamente para difundir propaganda o manipular la opinión pública a gran escala.
De manera similar a la difusión de información errónea, esto implica que ChatGPT genere respuestas falsas o engañosas que podrían considerarse erróneamente como objetivas, lo que afecta las decisiones comerciales y la confianza de los clientes.
Los sesgos inherentes a los datos de capacitación pueden generar resultados sesgados o prejuiciosos. Por ejemplo, si las respuestas distinguen entre grupos étnicos o géneros al tomar decisiones sobre contratación o ascensos. Esto podría llevar a una toma de decisiones poco ética y potencialmente generar problemas de relaciones públicas y ramificaciones legales.
Las empresas deben navegar por la delgada línea entre aprovechar las capacidades de ChatGPT para la productividad y garantizar que no dañen inadvertidamente a los consumidores a través de resultados sesgados o poco éticos. También deben asegurarse de que los empleados no incluyan PII o información confidencial del cliente en las solicitudes, lo que podría violar las normas de privacidad.
Si bien OpenAI realiza esfuerzos para reducir los sesgos, persiste el riesgo de que no todos los sesgos se aborden adecuadamente, lo que lleva a prácticas o resultados potencialmente discriminatorios.
ChatGPT puede utilizarse indebidamente para desarrollar sofisticados scripts de malware o ransomware, lo que plantea importantes amenazas a la seguridad de las empresas. Si bien va en contra de la política de OpenAI usar ChatGPT para ataques, la herramienta aún se puede manipular a través de varias indicaciones, como pedirle al chatbot que actúe como un probador de penetración o que escriba o depure scripts de código aparentemente no relacionados.
Como se mencionó anteriormente, ChatGPT se puede utilizar para generar código que pueda explotar vulnerabilidades en software o sistemas, facilitando el acceso no autorizado o la filtración de datos.
Los atacantes pueden utilizar ChatGPT para crear correos electrónicos de phishing muy convincentes, lo que aumenta la probabilidad de estafas exitosas y robo de información. Con esta herramienta de inteligencia artificial, pueden crear correos electrónicos que simulen tonos y voces, como figuras conocidas públicamente, parecer profesionales empresariales, como directores ejecutivos y TI, eliminar errores gramaticales, que son uno de los indicadores de correos electrónicos de phishing, y escribir en una amplia gama de idiomas, lo que les permite ampliar su espectro de ataque.
Al igual que los correos electrónicos de phishing, ChatGPT puede generar mensajes contextualmente relevantes y convincentes. Esto significa que puede utilizarse como arma para realizar ataques de ingeniería social, engañando a los empleados para que comprometan los protocolos de seguridad.
Las capacidades lingüísticas avanzadas de ChatGPT lo convierten en una herramienta para crear mensajes o contenido que se hace pasar por personas o entidades, lo que conduce a posibles fraudes e ingeniería social. y desinformación.
Se puede utilizar una generación de lenguaje sofisticado para elaborar mensajes que evadan la detección de los sistemas de moderación de contenido estándar. Esto plantea un riesgo para la seguridad y el cumplimiento en línea, ya que las herramientas de seguridad tradicionales son menos efectivas que antes.
La generación de contenido por parte de ChatGPT podría infringir inadvertidamente los derechos de propiedad intelectual existentes. Si ChatGPT crea contenido que refleja o se parece mucho a materiales protegidos por derechos de autor existentes, el resultado puede ser una infracción de propiedad intelectual, lo que plantea riesgos legales y financieros para las empresas.
La otra cara de la moneda es cuando ChatGPT proporciona respuestas basadas en su propia información patentada o contenido creativo, lo que genera pérdidas financieras y una desventaja competitiva.
El actor malicioso intenta eludir o explotar las salvaguardas integradas de OpenAI, con el objetivo de obligarlo a realizar tareas fuera de sus límites previstos o éticamente permitidos. Esto podría variar desde generar contenido que viole las políticas de uso hasta manipular el modelo para revelar información que está diseñado para ocultar. Dichos ataques podrían comprometer la integridad de los datos de las empresas que utilizan ChatGPT y han ingresado información confidencial, y hacerlas susceptibles a consecuencias comerciales y legales si utilizan datos incorrectos de las respuestas de ChatGPT.
Vulnerabilidades o fallas dentro del sistema que potencialmente podrían comprometer la privacidad del usuario. Estos podrían ser fallos que exponen accidentalmente datos confidenciales del usuario o lagunas que los actores malintencionados aprovechan para acceder a información no autorizada. Estos podrían comprometer la integridad empresarial, revelando planes de negocios, código fuente, información de clientes, información de empleados y más.
Los cambios en las políticas de OpenAI con respecto al uso de ChatGPT podrían tener implicaciones para las empresas que dependen de su tecnología. Dichos cambios podrían incluir modificaciones a las pautas de privacidad del usuario, las políticas de uso de datos o los marcos éticos que guían el desarrollo de la IA y despliegue. Desalineación entre estas nuevas políticas y las expectativas de los usuarios o los estándares legales, lo que podría generar problemas de privacidad, reducción de la confianza de los usuarios, desafíos legales y de cumplimiento, o desafíos con la continuidad operativa.
El uso de extensiones de ChatGPT, que son complementos o integraciones que amplían las capacidades de ChatGPT, también es un riesgo para la seguridad de ChatGPT. Éstos son algunos de los claves:
Hemos llegado a nuestra parte favorita: ¿qué hacer? Hay una manera de eCapacite a su fuerza laboral para aprovechar el inmenso potencial de productividad de ChatGPT y, al mismo tiempo, elimine su capacidad de exponer involuntariamente datos confidenciales. Así es cómo:
Determine los datos que más le preocupan: código fuente, planes de negocio, propiedad intelectual, etc. Establecer pautas sobre cómo y cuándo los empleados pueden usar ChatGPT, enfatizando los tipos de información que no deben compartirse con la herramienta o solo deben compartirse bajo condiciones estrictas.
Educar a los empleados sobre los riesgos y limitaciones potenciales del uso de herramientas de IA, que incluyen:
Promover una cultura en la que las herramientas de IA se utilicen de manera responsable como complemento de la experiencia humana, no como reemplazo.
Se accede a ChatGPT y se consume a través del navegador, como una aplicación web o una extensión del navegador. Por lo tanto, las herramientas tradicionales de seguridad de redes o terminales no se pueden utilizar para proteger las organizaciones y evitar que los empleados peguen o escriban datos confidenciales en aplicaciones GenAI.
Pero un extensión del navegador empresarial poder. Al crear una política ChatGPT dedicada, el navegador puede evitar el intercambio de datos confidenciales mediante advertencias emergentes o bloquear el uso por completo. En casos extremos, el navegador empresarial se puede configurar para desactivar ChatGPT y sus extensiones por completo.
Escanee los navegadores de su fuerza laboral para descubrir los instalados extensiones maliciosas de ChatGPT eso debería ser eliminado. Además, analice continuamente el comportamiento de las extensiones de navegador existentes para evitar que accedan a datos confidenciales del navegador. Deshabilite la capacidad de las extensiones para extraer credenciales u otros datos confidenciales de los navegadores de su fuerza laboral.
Dada la capacidad de los atacantes para utilizar ChatGPT en su beneficio, haga de la ciberseguridad una mayor prioridad. Esto incluye:
LayerX es una solución de navegador empresarial que protege a las organizaciones contra amenazas y riesgos transmitidos por la web. LayerX tiene una solución única para proteger a las organizaciones contra la exposición de datos confidenciales a través de ChatGPT y otras herramientas de inteligencia artificial generativa, sin interrumpir la experiencia del navegador.
Los usuarios pueden mapear y definir los datos a proteger, como el código fuente o la propiedad intelectual. Cuando los empleados usan ChatGPT, se aplican controles como advertencias emergentes o bloqueos para garantizar que no se expongan datos seguros. LayerX garantiza una productividad segura y la utilización total del potencial de ChatGPT sin comprometer la seguridad de los datos.
Para más detalles, haga clic aquí.
O Eshed es cofundador y director ejecutivo de la plataforma Browser Security LayerX, con más de una década de experiencia en ciberseguridad, inteligencia artificial y guerra de información.
Copyright © 2024 LayerX EULA Política de Privacidad Programa de divulgación de vulnerabilidades
Utilizamos cookies para asegurarnos de que nuestro sitio web funcione sin problemas y para mejorar su experiencia con nosotros. Al continuar navegando, aceptas el uso de cookies. Para obtener más información, consulte nuestro política de privacidad.
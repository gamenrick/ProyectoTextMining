Inteligencia artificial: entre el mito y la realidad | El Correo de la UNESCO
Autor desconocido
unesco.org

El Correo de la UNESCO
El Correo de la UNESCO
Artículo
Por Jean-Gabriel Ganascia
La inteligencia artificial (IA) es una disciplina científica que nació oficialmente en 1956 en el Dartmouth College, en Hanover (Estados Unidos), durante un curso de verano organizado por cuatro investigadores estadounidenses: John McCarthy, Marvin Minsky, Nathaniel Rochester y Claude Shannon. Desde entonces, la expresión “inteligencia artificial”, que al principio fue inventada probablemente para llamar la atención, se ha vuelto tan popular que hoy día todos saben de qué se trata. Este componente de la informática ha crecido de forma constante con el paso del tiempo y las tecnologías que de ella se derivan han contribuido en gran medida a transformar el mundo durante los últimos sesenta años.
Sin embargo, el éxito de la expresión “inteligencia artificial” se debe a veces a un malentendido cuando se la utiliza para referirse a un ente artificial dotado de inteligencia y, por lo tanto, capaz de rivalizar con el ser humano. Esta idea, que remite a mitos y leyendas antiguas, como la del Golem, fue reactivada recientemente por personalidades del mundo contemporáneo como el físico británico Stephen Hawking (1942-2018), el empresario estadounidense Elon Musk, el futorólogo estadounidense Ray Kurzweil o inclusive por defensores de lo que hoy se denomina la “ IA fuerte” o la “IA general”. Sin embargo, no entraremos aquí en más detalles sobre esta segunda vertiente, ya que es producto únicamente de una fértil imaginación, inspirada más por la ciencia ficción que por una realidad científica tangible confirmada por experimentos y observaciones empíricas.
Para John McCarthy y Marvin Minsky, como para los demás organizadores del curso de verano del Dartmouth College, la IA tiene por objetivo inicialmente la simulación con máquinas de cada una de las distintas facultades de la inteligencia, ya sea de la inteligencia humana, animal, vegetal, social o filogenética. Más precisamente, esta disciplina científica se basó en la suposición de que todas las funciones cognitivas, en especial el aprendizaje, el razonamiento, el cálculo, la percepción, la memorización e incluso el descubrimiento científico o la creatividad artística pueden describirse con una precisión tal que sería posible programar un ordenador para reproducirlas. Hace más de sesenta años que la IA existe y no ha habido nada que permita desmentir o demostrar de manera irrefutable esta especulación, que sigue siendo válida y fecunda.
En el transcurso de su breve existencia, la IA experimentó numerosas transformaciones. Podemos resumirlas en seis etapas.
En un principio, con la euforia de los orígenes y de los primeros éxitos, los investigadores dieron rienda suelta a su imaginación con algunas declaraciones precipitadas, por las cuales han sido criticados severamente desde entonces. Por ejemplo, en 1958, el estadounidense Herbert Simon, quien posteriormente sería galardonado con el Premio Nobel de Economía, había declarado que dentro de diez años las máquinas serían campeonas del mundo de ajedrez, a menos que se las excluyera de las competencias internacionales.
A mediados de la década de 1960, los avances tardaron en hacerse sentir. Un niño de diez años derrotó a una computadora en una partida de ajedrez en 1965. Un informe encargado por el Senado de los Estados Unidos daba cuenta, en 1966, de las limitaciones intrínsecas de la traducción automática. La IA tuvo entonces mala prensa durante una década.
Sin embargo, los trabajos no se interrumpieron, pero las investigaciones tomaron nuevos rumbos. El interés se centró en la psicología de la memoria, en los mecanismos de comprensión, que se procuró simular en un ordenador, y en el papel del conocimiento en el razonamiento. Esto dio origen a las técnicas de representación semántica de los conocimientos, que se desarrollaron considerablemente a mediados de la década de 1970 y también contribuyeron al desarrollo de los sistemas expertos, así denominados porque utilizan el conocimiento de especialistas cualificados para reproducir sus razonamientos. Estos sistemas despertaron grandes esperanzas a comienzos de la década de 1980 con múltiples aplicaciones, por ejemplo, para el diagnóstico médico.
El perfeccionamiento de las técnicas condujo a la elaboración de algoritmos de aprendizaje automático (machine learning), que permitieron a los ordenadores acumular conocimientos y reprogramarse automáticamente a partir de sus propias experiencias.
Esto dio origen a aplicaciones industriales (identificación de huellas dactilares, reconocimiento de voz, etc.), donde las técnicas derivadas de la IA, de la informática, de la vida artificial y de otras disciplinas conviven para crear sistemas híbridos.
Desde finales de los años 90, la IA se acopló a la robótica y a las interfaces hombre-máquina a fin de crear agentes inteligentes que sugieren la presencia de afectos y de emociones. Esto dio origen, entre otros, a la informática emocional (affective computing), que evalúa las reacciones de un sujeto que experimenta emociones y las reproduce en una máquina, y en especial al perfeccionamiento de los programas informáticos que simulan conversaciones con usuarios humanos (chatbot).
Desde 2010, la potencia de las máquinas permite aprovechar los macrodatos o inteligencia de datos (big data) con técnicas de aprendizaje profundo (deep learning), que se basan en el uso de redes neuronales formales. Algunas aplicaciones muy prometedoras en diversas áreas (reconocimiento de voz, de imágenes, comprensión del lenguaje natural, vehículos autónomos, etc.) hacen pensar en un resurgimiento de la IA.
Muchas de los productos que utilizan técnicas de IA superan las capacidades humanas: en 1997 una máquina venció al campeón mundial de ajedrez y, más recientemente, en 2016, otras derrotaron a uno de los mejores jugadores del mundo al juego del Go y a excelentes jugadores de póker; los ordenadores demuestran o ayudan a demostrar teoremas matemáticos; se adquieren automáticamente conocimientos a partir de cantidades inmensas de datos, cuyo volumen se mide en terabytes (1012 bytes) o incluso en petabytes (1015 bytes), utilizando técnicas de aprendizaje automático.
Gracias a estas técnicas, las máquinas reconocen la palabra articulada y la transcriben, como las secretarias mecanógrafas de antaño, y otras identifican con precisión rostros o huellas dactilares entre decenas de millones y comprenden textos escritos en lenguaje natural. Siempre gracias a estas técnicas de aprendizaje automático, los vehículos se conducen solos, las máquinas diagnostican mejor que los médicos dermatólogos los melanomas utilizando fotografías de lunares tomadas sobre la piel con teléfonos móviles, los robots luchan en la guerra en lugar de los humanos y las cadenas de producción en las fábricas se automatizan cada vez más.
Además, los científicos utilizan estas técnicas para determinar la función de algunas macromoléculas biológicas, en especial de proteínas y de genomas, a partir de la secuencia de sus componentes –aminoácidos para las proteínas, bases para los genomas. De manera más general, todas las ciencias experimentan una ruptura epistemológica importante con los experimentos denominados in silico, porque éstos se efectúan a partir de cantidades masivas de datos, utilizando procesadores potentes, cuyo núcleo está hecho de silicio, en contraposición con los experimentos in vivo, en la materia viva, y, sobre todo, in vitro, es decir en probetas de vidrio.
Estas aplicaciones de la IA influyen en casi todas las áreas de actividad, especialmente en los sectores de la industria, la banca, los seguros, la salud y la defensa. Muchas tareas rutinarias ahora pueden ser automatizadas, transformando algunos empleos y eliminando eventualmente otros.
Con la IA, no sólo la mayoría de las dimensiones de la inteligencia ‒salvo tal vez el humor‒ son objeto de análisis y de reconstrucciones racionales con ordenadores, sino que además las máquinas traspasan nuestras facultades cognitivas en la mayoría de los terrenos, lo cual despierta temores de riesgos de carácter ético. Estos riesgos son de tres órdenes: la escasez de trabajo, que sería ejecutado por máquinas en lugar de seres humanos; las consecuencias para la autonomía del individuo, en especial para su libertad y su seguridad; y la superación del género humano, que sería sustituido por máquinas cada vez más “inteligentes”.
Sin embargo, un examen detallado muestra que el trabajo no desaparece, sino que, muy al contrario, se transforma y exige nuevas habilidades. Del mismo modo, la autonomía del individuo y su libertad no están inexorablemente comprometidas por el desarrollo de la IA, siempre y cuando nos mantengamos alerta en lo relativo a las intromisiones de la tecnología en nuestra vida privada.
Por último, contrariamente a lo que algunos piensan, las máquinas no constituyen de ningún modo una amenaza existencial para la humanidad, ya que su autonomía es de carácter meramente técnico, en el sentido de que corresponde sólo a las cadenas materiales de causalidades, que van desde la búsqueda de información hasta la toma de decisiones. En cambio, las máquinas no tienen autonomía moral, puesto que, si bien podrían despistarnos y confundirnos en el momento de actuar, no poseen voluntad propia y permanecen al servicio de los objetivos que les hemos fijado.
Foto: Max Aguilera-Hellweg
Jean-Gabriel Ganascia
Profesor de informática en la Universidad de la Sorbona, Jean-Gabriel Ganascia (Francia) también es investigador en el LIP6, miembro de la Asociación Europea de Inteligencia Artificial (EurAI), miembro del Institut Universitaire de France y presidente del comité de ética del Centro Nacional para la Investigación Científica (CNRS) de Francia. Sus actividades de investigación abarcan actualmente el aprendizaje automático, la fusión simbólica de datos, la ética informática y las humanidades digitales.
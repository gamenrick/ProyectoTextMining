¿Qué es el deep learning? | IBM
Autor desconocido
ibm.com

Actualizado: 17 junio 2024
Colaboradores: Jim Holdsworth, Mark Scapicchio
El deep learning es un subconjunto del machine learning que utiliza redes neuronales multicapa, llamadas redes neuronales profundas, para simular el complejo poder de toma de decisiones del cerebro humano. Algunas formas de deep learning impulsan la mayoría de las aplicaciones de inteligencia artificial (IA) en nuestra vida actual.
La principal diferencia entre el deep learning y el machine learning es la estructura de la arquitectura de red neuronal subyacente. Los modelos tradicionales de machine learning “no profundos” utilizan redes neuronales simples con una o dos capas computacionales. Los modelos de deep learning utilizan tres o más capas, pero normalmente cientos o miles de capas, para entrenar los modelos.
Mientras que los modelos de aprendizaje supervisado requieren datos de entrada estructurados y etiquetados para obtener resultados precisos, los modelos de deep learning pueden utilizar el aprendizaje no supervisado. Con el aprendizaje no supervisado, los modelos de deep learning pueden extraer las características, los rasgos y las relaciones que necesitan para obtener resultados precisos a partir de datos brutos y no estructurados. Además, estos modelos pueden incluso evaluar y refinar sus resultados para aumentar la precisión.
El deep learning es un aspecto de la ciencia de datos que impulsa muchas aplicaciones y servicios que mejoran la automatización, realizando tareas analíticas y físicas sin intervención humana. Esto permite muchos productos y servicios cotidianos, como asistentes digitales, controles remotos de TV habilitados para voz, detección de fraudes con tarjetas de crédito, automóviles autónomos e IA generativa. 
Descubra los componentes básicos y las buenas prácticas para ayudar a sus equipos a acelerar la IA responsable. 
Regístrese para recibir el libro electrónico sobre IA generativa
Las redes neuronales, o redes neuronales artificiales, intentan imitar el cerebro humano a través de una combinación de entradas de datos, ponderaciones y sesgos, todos actuando como neuronas de silicio. Estos elementos trabajan juntos para reconocer, clasificar y describir con precisión los objetos dentro de los datos.
Las redes neuronales profundas se componen de varias capas de nodos interconectados, cada una de las cuales se basa en la capa anterior para refinar y optimizar la predicción o la categorización. Esta progresión de cálculos a través de la red se denomina propagación hacia adelante. Las capas de entrada y salida de una red neuronal profunda se denominan capas visibles . La capa de entrada es donde el modelo de deep learning ingiere los datos para su procesamiento, y la capa de salida es donde se realiza la predicción o clasificación final.
Otro proceso llamado retropropagación utiliza algoritmos, como el descenso de gradientes, para calcular los errores en las predicciones y, a continuación, ajusta las ponderaciones y los sesgos de la función moviéndose hacia atrás por las capas para entrenar el modelo. Juntas, la propagación progresiva y la retropropagación permiten a una red neuronal hacer predicciones y corregir errores. Con el tiempo, el algoritmo se vuelve gradualmente más preciso.

El deep learning requiere una enorme cantidad de potencia informática. Las unidades de procesamiento gráfico (GPU) de alto rendimiento son ideales porque pueden manejar un gran volumen de cálculos en varios núcleos con memoria copiosa disponible. El cloud computing distribuida también podría resultar útil. Este nivel de potencia de cómputo es necesario para entrenar algoritmos profundos a través del deep learning. Sin embargo, gestionar varias GPU en las instalaciones puede generar una gran demanda de recursos internos y su escalamiento es increíblemente caro. Para los requisitos de software, la mayoría de las aplicaciones de deep learning están codificadas con uno de estos tres marcos de aprendizaje: JAX, PyTorch o TensorFlow.

 
Los algoritmos de deep learning son increíblemente complejos y hay diferentes tipos de redes neuronales para abordar problemas o conjuntos de datos específicos. Aquí hay seis. Cada uno tiene sus propias ventajas y se presentan aquí aproximadamente en el orden de su desarrollo, con cada modelo sucesivo ajustándose para superar una debilidad de un modelo anterior.
Un posible punto débil de todos ellos es que los modelos de deep learning suelen ser "cajas negras", lo que dificulta la comprensión de su funcionamiento interno y plantea problemas de interpretabilidad. Pero esto se puede equilibrar con los beneficios generales de la alta precisión y escalabilidad.
Las redes neuronales convolucionales (CNN o ConvNets) se utilizan principalmente en aplicaciones de visión artificial y clasificación de imágenes. Pueden detectar características y patrones dentro de imágenes y videos, lo que permite tareas como la detección de objetos, el reconocimiento de imágenes, el reconocimiento de patrones y el reconocimiento facial. Estas redes aprovechan principios del álgebra lineal, en particular la multiplicación de matrices, para identificar patrones dentro de una imagen.
Las CNN son un tipo específico de red neuronal, que se compone de capas de nodos, que contienen una capa de entrada, una o más capas ocultas y una capa de salida. Cada nodo está conectado a otro y tiene un peso y un umbral asociados. Si la salida de cualquier nodo individual está por encima del valor umbral especificado, ese nodo se activa y envía datos a la siguiente capa de la red. De lo contrario, no se pasa ningún dato a la siguiente capa de la red.
Hay al menos tres tipos principales de capas que componen una CNN: una capa convolucional, una capa de agrupación y una capa totalmente conectada (FC). Para usos complejos, una CNN puede contener hasta miles de capas, cada una de las cuales se basa en las capas anteriores. Mediante la “convolución” (trabajar y reelaborar la entrada original) se pueden descubrir patrones detallados. Con cada capa, la CNN aumenta su complejidad, identificando mayores porciones de la imagen. Las primeras capas se centran en características simples, como colores y bordes. A medida que los datos de la imagen avanzan a través de las capas, la CNN comienza a reconocer elementos o formas más grandes hasta que finalmente identifica el objeto esperado.
Las CNN se distinguen de otras redes neuronales por su rendimiento superior con entradas de señal de imagen, voz o audio. Antes de las CNN, se utilizaban métodos manuales de extracción de características que requerían mucho tiempo para identificar objetos en imágenes. Sin embargo, las CNN ofrecen ahora un enfoque más escalable para las tareas de clasificación de imágenes y reconocimiento de objetos, y procesan datos de alta dimensión. Y las CNN pueden intercambiar datos entre capas, para ofrecer un procesamiento de datos más eficiente. Si bien la información puede perderse en la capa de agrupación, esto podría verse compensado por los beneficios de las CNN, que pueden ayudar a reducir la complejidad, mejorar la eficiencia y limitar el riesgo de sobreajuste. 
Las CNN tienen otras desventajas, que son muy exigentes desde el punto de vista computacional, ya que cuestan tiempo y presupuesto, ya que requieren muchas unidades de procesamiento gráfico (GPU). También requieren expertos altamente cualificados con conocimientos de distintos ámbitos y pruebas minuciosas de configuraciones, hiperparámetros y configuraciones.
Redes neuronales recurrentes (RNN) se utilizan normalmente en aplicaciones de lenguaje natural y reconocimiento de voz, ya que utilizan datos secuenciales o de series temporales. Las RNN se pueden identificar por sus bucles de comentarios. Estos algoritmos de aprendizaje se utilizan principalmente cuando se utilizan datos de series temporales para hacer predicciones sobre resultados futuros. Los casos de uso incluyen predicciones bursátiles o de ventas, o problemas ordinales o temporales, como traducción del idioma, procesamiento del lenguaje natural (PLN), reconocimiento de voz y leyendas de imágenes. Estas funciones a menudo se incorporan en aplicaciones populares como Siri, búsqueda por voz y Google Translate.
Las RNN utilizan su "memoria", ya que toman información de entradas anteriores para influir en la entrada y la salida actuales. Si bien las redes neuronales profundas tradicionales asumen que las entradas y las salidas son independientes entre sí, la salida de los RNN depende de los elementos anteriores de la secuencia. Mientras que las redes neuronales profundas tradicionales asumen que las entradas y las salidas son independientes entre sí, la salida de las RNN depende de los elementos anteriores dentro de la secuencia.
Las RNN comparten parámetros en cada capa de la red y tienen el mismo parámetro de peso dentro de cada capa de la red, con los pesos ajustados mediante los procesos de retropropagación y descenso de gradiente para facilitar el aprendizaje por refuerzo.
Las RNN utilizan un algoritmo de retropropagación en el tiempo (BPTT) para determinar los gradientes, que es ligeramente diferente de la retropropagación tradicional, ya que es específica para los datos secuenciales. Los principios de la BPTT son los mismos que los de la retropropagación tradicional, en la que el modelo se entrena a sí mismo calculando los errores de su capa de salida a su capa de entrada. BPTT difiere del enfoque tradicional en que BPTT suma errores en cada paso de tiempo, mientras que las redes feedforward no necesitan sumar errores ya que no comparten parámetros en cada capa.
Una ventaja sobre otros tipos de redes neuronales es que las RNN utilizan tanto el procesamiento de datos binarios como la memoria. Las RNN pueden planificar múltiples entradas y producciones de modo que, en lugar de ofrecer un único resultado para una sola entrada, las RMM pueden producir salidas de una a varias, de varias a una o de varias a varias.

También hay opciones dentro de las RNN. Por ejemplo, la red de memoria a corto plazo (LSTM) es superior a las RNN simples al aprender y actuar sobre las dependencias a largo plazo.
Sin embargo, las RNN tienden a enfrentarse a dos problemas básicos, conocidos como gradientes de explosión y gradientes de desaparición. Estos problemas se definen por el tamaño del gradiente, que es la pendiente de la función de pérdida a lo largo de la curva de error.
Algunas desventajas finales: las RNN también pueden requerir un largo tiempo de entrenamiento y ser difíciles de usar en grandes conjuntos de datos. La optimización de RNN agrega complejidad cuando tienen muchas capas y parámetros.
El deep learning permitió ir más allá del análisis de datos numéricos, añadiendo el análisis de imágenes, voz y otros tipos de datos complejos. Entre la primera clase de modelos en lograrlo se encuentran los autocodificadores variacionales (VAE). Fueron los primeros modelos de deep learning que se utilizaron ampliamente para generar imágenes y voz realistas, lo que potenció el modelado generativo profundo al facilitar el escalado de los modelos, que es la piedra angular de lo que consideramos la IA generativa.
Los autocodificadores codifican los datos no etiquetados en una representación comprimida y luego los descodifican en su forma original. Los autocodificadores simples se utilizaban para diversos fines, incluida la reconstrucción de imágenes corruptas o borrosas. Los autocodificadores variacionales añadían la capacidad crucial no solo de reconstruir los datos, sino también de generar variaciones de los datos originales.
Esta capacidad de generar nuevos datos dio lugar a una rápida sucesión de nuevas tecnologías, desde las redes generativas antagónicas (GAN) hasta los modelos de difusión, capaces de producir imágenes cada vez más realistas, aunque falsas. De esta manera, los VAE preparan el escenario para la IA generativa actual.
Los autocodificadores se construyen a partir de bloques de codificadores y decodificadores, una arquitectura que también sustenta los modelos de lenguaje de gran tamaño actuales. Los codificadores comprimen un conjunto de datos en una representación densa, organizando puntos de datos similares más juntos en un espacio abstracto. Los decodificadores toman muestras de este espacio para crear algo nuevo al tiempo que conservan las características más importantes del conjunto de datos.
La mayor ventaja de los autocodificadores es su capacidad para gestionar grandes lotes de datos y mostrar los datos de entrada de forma comprimida, por lo que destacan los aspectos más significativos, como la detección de anomalías y las tareas de clasificación. Esto también acelera la transmisión y reduce los requisitos de almacenamiento. Los autocodificadores se pueden entrenar con datos sin etiquetar para que puedan usarse cuando los datos etiquetados no estén disponibles. Cuando se utiliza el entrenamiento no supervisado, existe una ventaja de ahorro de tiempo: los algoritmos de deep learning aprenden automáticamente y ganan precisión sin necesidad de ingeniería manual de características. Además, los VAE pueden generar nuevos datos de muestra para la generación de texto o imágenes.
Los autocodificadores tienen sus desventajas. El entrenamiento de estructuras profundas o intrincadas puede ser una pérdida de recursos computacionales. Y durante el entrenamiento no supervisado, el modelo puede pasar por alto las propiedades necesarias y, en su lugar, simplemente replicar los datos de entrada. Los autocodificadores también pueden pasar por alto vínculos de datos complejos en datos estructurados, de modo que no identifiquen correctamente las relaciones complejas.
Las redes generativas antagónicas (GAN) son redes neuronales que se utilizan tanto dentro como fuera de la inteligencia artificial (IA) para crear nuevos datos que se parezcan a los datos de entrenamiento originales. Pueden incluir imágenes que parezcan rostros humanos, pero son generadas, no tomadas de personas reales. La parte "antagónica" del nombre proviene del vaivén entre las dos partes del GAN: un generador y un discriminador.
Las GAN se entrenan solas. El generador crea falsificaciones mientras el discriminador aprende a detectar las diferencias entre las falsificaciones del generador y los ejemplos reales. Cuando el discriminador es capaz de marcar la falsificación, se penaliza al generador. El bucle de retroalimentación continúa hasta que el generador logra producir una salida que el discriminador no puede distinguir.
La principal ventaja de las GAN es la creación de resultados realistas que pueden ser difíciles de distinguir de los originales, lo que a su vez puede utilizarse para entrenar más modelos de machine learning. Configurar un GAN para que aprenda es sencillo, ya que se entrenan utilizando datos sin etiquetar o con un etiquetado menor. Sin embargo, la desventaja potencial es que el generador y el discriminador podrían ir y venir compitiendo durante mucho tiempo, creando un gran drenaje del sistema. Una limitación del entrenamiento es que puede ser necesaria una enorme cantidad de datos de entrada para obtener un resultado satisfactorio. Otro problema potencial es el "colapso de modo", cuando el generador produce un conjunto limitado de salidas en lugar de una variedad más amplia.
Los modelos de difusión son modelos generativos que se entrenan utilizando el proceso de difusión directa e inversa de adición progresiva de ruido y eliminación de ruido. Los modelos de difusión generan datos (la mayoría de las veces imágenes) similares a los datos con los que se entrenan, pero luego sobrescriben los datos utilizados para entrenarlos. Añaden gradualmente ruido gaussiano a los datos de entrenamiento hasta hacerlos irreconocibles y, a continuación, aprenden un proceso de "eliminación de ruido" inverso que puede sintetizar la salida (normalmente imágenes) a partir de la entrada de ruido aleatorio.
Un modelo de difusión aprende a minimizar las diferencias de las muestras generadas frente al objetivo deseado. Se cuantifica cualquier discrepancia y se actualizan los parámetros del modelo para minimizar la pérdida, entrenando al modelo para que produzca muestras muy parecidas a los datos de entrenamiento auténticos.
Más allá de la calidad de la imagen, los modelos de difusión tienen la ventaja de no requerir entrenamiento de adversarios, lo que acelera el proceso de aprendizaje y también ofrece un control estricto del proceso. El entrenamiento es más estable que con las GAN y los modelos de difusión no son tan propensos al colapso de modos.
Pero, en comparación con las GAN, los modelos de difusión pueden requerir más recursos informáticos para entrenarse, incluido un mayor ajuste. IBM Research también ha descubierto que esta forma de IA generativa puede ser secuestrada con puertas traseras ocultas, lo que da a los atacantes el control sobre el proceso de creación de imágenes para que los modelos de difusión de IA puedan ser engañados para generar imágenes manipuladas.
Los modelos de transformador combinan una arquitectura de codificador-decodificador con un mecanismo de procesamiento de texto y han revolucionado la forma en que se entrenan los modelos de lenguaje. Un codificador convierte el texto sin procesar, sin anotar, en representaciones conocidas como incrustaciones; el descodificador toma estas incrustaciones junto con los resultados anteriores del modelo y predice sucesivamente cada palabra de una frase.
Mediante adivinanzas para completar espacios en blanco, el codificador aprende cómo se relacionan entre sí las palabras y las frases, construyendo una potente representación del lenguaje sin tener que etiquetar las partes de la oración y otros rasgos gramaticales. Los transformadores, de hecho, se pueden entrenar previamente desde el principio sin una tarea particular en mente. Una vez que se aprenden estas poderosas representaciones, los modelos se pueden especializar, con muchos menos datos, para realizar una tarea solicitada.
Varias innovaciones lo hacen posible. Los transformadores procesan las palabras de una oración simultáneamente, lo que permite procesar el texto en paralelo y acelera el entrenamiento. Las técnicas anteriores, incluidas las redes neuronales recurrentes (RNN), procesaban las palabras una por una. Los transformadores también aprendieron las posiciones de las palabras y sus relaciones: este contexto les permite inferir significados y desambiguar palabras como "eso" en frases largas.
Al eliminar la necesidad de definir una tarea por adelantado, los transformadores hicieron que fuera práctico entrenar previamente los modelos de lenguaje en grandes cantidades de texto sin formato, lo que les permitió crecer dramáticamente en tamaño. Anteriormente, los datos etiquetados se recopilaban para entrenar un modelo en una tarea específica. Con los transformadores, un modelo entrenado con una gran cantidad de datos se puede adaptar a múltiples tareas ajustándolo en una pequeña cantidad de datos etiquetados específicos de la tarea.
Hoy en día, los transformadores de lenguaje se utilizan para tareas no generativas, como la clasificación y la extracción de entidades, así como para tareas generativas, como la traducción automática, el resumen y la respuesta a preguntas. Los transformadores han sorprendido a muchas personas con su capacidad para generar diálogos, ensayos y otros contenidos convincentes.
Los transformadores de procesamiento del lenguaje natural (PLN) proporcionan una potencia notable, ya que pueden ejecutarse en paralelo, procesando múltiples partes de una secuencia simultáneamente, lo que acelera enormemente el entrenamiento. Los transformadores también rastrean las dependencias a largo plazo en el texto, lo que les permite entender el contexto general con mayor claridad y crear resultados superiores. Además, los transformadores son más escalables y flexibles para personalizarlos según las tareas.
En cuanto a las limitaciones, debido a su complejidad, los transformadores requieren enormes recursos computacionales y un largo tiempo de entrenamiento. Además, los datos de entrenamiento deben ser precisos, imparciales y abundantes para producir resultados exactos.
El número de usos del deep learning crece cada día. He aquí algunas de las formas en las que ayuda a las empresas a ser más eficientes y servir mejor a sus clientes.
La IA generativa puede mejorar las capacidades de los desarrolladores y reducir la brecha de habilidades cada vez mayor en los ámbitos de la modernización de aplicaciones y la automatización de TI. La IA generativa para la codificación es posible gracias a los recientes avances en las tecnologías de modelos de lenguaje de gran tamaño (LLM) y el procesamiento del lenguaje natural (PLN). Utiliza algoritmos de deep learning y grandes redes neuronales entrenadas en amplios conjuntos de datos de código fuente existente. El código de entrenamiento suele proceder de código disponible públicamente producido por proyectos de código abierto.
Los programadores pueden introducir mensajes de texto sin formato que describan lo que quieren que haga el código. Las herramientas de IA generativa sugieren fragmentos de código o funciones completas, lo que agiliza el proceso de codificación mediante el manejo de tareas repetitivas y reduce la codificación manual. La IA generativa también puede traducir código de un lenguaje a otro, con lo que se agilizan los proyectos de conversión o modernización de código, como la actualización de aplicaciones heredadas mediante la traducción de COBOL a Java.
La visión artificial es un campo de la inteligencia artificial (IA) que incluye la clasificación de imágenes, la detección de objetos y la segmentación semántica. Utiliza el machine learning y las redes neuronales para enseñar a ordenadores y sistemas de aprendizaje a extraer información significativa de imágenes digitales, vídeos y otras entradas visuales, y a hacer recomendaciones o tomar medidas cuando el sistema detecta defectos o problemas. Si la IA permite a los ordenadores pensar, la visión artificial les permite ver, observar y comprender.
Dado que un sistema de visión por ordenador suele estar entrenado para inspeccionar productos o vigilar activos de producción, normalmente puede analizar miles de productos o procesos por minuto, detectando defectos o problemas imperceptibles. La visión artificial se utiliza en industrias que van desde la energía y los servicios públicos hasta la fabricación y la automoción.
La visión por ordenador necesita muchos datos, y después realiza análisis de esos datos una y otra vez hasta que discierne y, en última instancia, reconoce las imágenes. Por ejemplo, para entrenar a un ordenador en el reconocimiento de neumáticos de automóvil, es necesario alimentarlo con grandes cantidades de imágenes de neumáticos y elementos relacionados con ellos para que aprenda las diferencias y reconozca un neumático, especialmente uno sin defectos.
La visión artificial utiliza modelos algorítmicos para permitir que un ordenador se enseñe a sí mismo sobre el contexto de los datos visuales. Si se introducen suficientes datos en el modelo, el ordenador "mirará" los datos y aprenderá a distinguir una imagen de otra. Los algoritmos permiten que la máquina aprenda por sí misma, en lugar de que alguien la programe para reconocer una imagen.
La visión artificial permite a los sistemas obtener información significativa a partir de imágenes digitales, vídeos y otras entradas visuales y, basándose en ellas, emprender acciones. Esta capacidad de asesoramiento la distingue de las actividades de reconocimiento de imágenes simples. Hoy en día, se pueden encontrar algunas aplicaciones comunes de la visión artificial en las siguientes áreas:

La IA está ayudando a las empresas a comprender mejor y satisfacer las crecientes demandas de los consumidores. Con el auge de las compras en línea altamente personalizadas, los modelos directos al consumidor y los servicios de entrega, la IA generativa puede ayudar a desbloquear aún más una serie de beneficios que pueden mejorar la atención al cliente, la transformación del talento y el rendimiento de las aplicaciones.
La IA permite a las empresas adoptar un enfoque centrado en el cliente aprovechando la valiosa información que aportan sus comentarios y hábitos de compra. Este enfoque basado en los datos puede ayudar a mejorar el diseño y el envasado de los productos, así como a conseguir un alto grado de satisfacción de los clientes y un aumento de las ventas.
La IA generativa también puede servir como asistente cognitivo para la atención al cliente, proporcionando orientación contextual basada en el historial de conversaciones, el análisis de sentimientos y las transcripciones del centro de llamadas. Además, la IA generativa puede permitir experiencias de compra personalizadas, fomentar la lealtad de los clientes y proporcionar una ventaja competitiva.
Las organizaciones pueden aumentar su plantilla creando e implementando la automatización de procesos robóticos (RPA) y el trabajo digital para colaborar con los humanos para aumentar la productividad o ayudar cuando se necesite una copia de seguridad. Por ejemplo, esto puede ayudar a los desarrolladores a acelerar la actualización del software heredado.
El trabajo digital utiliza modelos fundacionales para automatizar y mejorar la productividad de los trabajadores del conocimiento al permitir la automatización del autoservicio de una manera rápida y confiable, sin barreras técnicas. Para automatizar la realización de tareas o la llamada a API, un modelo de llenado de huecos basado en LLM de nivel empresarial puede identificar información en una conversación y recopilar toda la información necesaria para completar una acción o llamar a una API sin mucho esfuerzo manual.
En lugar de que los expertos técnicos registren y codifiquen flujos de acciones repetitivas para los trabajadores del conocimiento, los trabajadores del conocimiento pueden utilizar automatizaciones digitales del trabajo creadas con una base de instrucciones y demostraciones conversacionales basadas en modelos para la automatización del autoservicio. Por ejemplo, para acelerar la creación de aplicaciones, los aprendices digitales sin código pueden ayudar a los usuarios finales, que carecen de conocimientos en programación, enseñando, supervisando y validando eficazmente el código. 
La IA generativa (también llamada IA gen) es una categoría de IA que crea de forma autónoma texto, imágenes, vídeo, datos u otro contenido en respuesta a la solicitud o solicitud de un usuario.
La IA generativa se basa en modelos de deep learning que pueden aprender de patrones en el contenido existente y generar contenido nuevo y similar basado en ese entrenamiento. Tiene aplicaciones en muchos campos, incluidos el servicio al cliente, el marketing, el desarrollo de software y la investigación, y ofrece un enorme potencial para optimizar los flujos de trabajo empresariales a través de la creación y el aumento de contenido rápido y automatizado. 
La IA generativa destaca en el manejo de diversas fuentes de datos, como correos electrónicos, imágenes, vídeos, archivos de audio y contenidos de redes sociales. Estos datos no estructurados forman la columna vertebral para la creación de modelos y el entrenamiento continuo de la IA generativa, para que pueda seguir siendo eficaz a lo largo del tiempo. El uso de estos datos no estructurados puede mejorar el servicio al cliente a través de chatbots y facilitar un enrutamiento de correo electrónico más efectivo. En la práctica, esto podría significar guiar a los usuarios a los recursos adecuados, ya sea conectándolos con el agente adecuado o dirigiéndolos a guías de usuario y preguntas frecuentes.
A pesar de sus limitaciones y riesgos tan discutidos, muchas empresas están avanzando, explorando con cautela cómo sus organizaciones pueden aprovechar la IA generativa para mejorar sus flujos de trabajo internos y mejorar sus productos y servicios. Esta es la nueva frontera: cómo hacer que el lugar de trabajo sea más eficiente sin crear problemas legales o éticos.

El PLN combina la lingüística computacional (modelado basado en reglas del lenguaje humano) con modelos estadísticos y de machine learning para permitir que las computadoras y los dispositivos digitales reconozcan, comprendan y generen texto y voz. El PLN impulsa aplicaciones y dispositivos que pueden traducir texto de un idioma a otro, responder a comandos escritos o hablados, reconocer o autenticar usuarios en función de la voz.  Ayuda a resumir grandes volúmenes de texto, evaluar la intención o el sentimiento del texto o la voz y generar texto o gráficos u otro contenido bajo demanda.

Un subconjunto del PNL es el PNL estadístico, que combina algoritmos informáticos con modelos de machine learning y deep learning. Este enfoque ayuda a extraer, clasificar y etiquetar automáticamente elementos de datos de texto y voz y, a continuación, asignar una probabilidad estadística a cada posible significado de esos elementos. Hoy en día, los modelos de deep learning y las técnicas de aprendizaje basadas en RNN permiten que los sistemas de PLN "aprendan" mientras trabajan y extraigan significados cada vez más precisos de enormes volúmenes de conjuntos de datos de texto y voz sin procesar, sin estructurar y sin etiquetar.
El reconocimiento de voz, también conocido como reconocimiento automático de voz (ASR), reconocimiento de voz por ordenador o conversión de voz a texto, es una capacidad que permite a un programa procesar el habla humana en un formato escrito.
Aunque el reconocimiento del habla suele confundirse con el reconocimiento de la voz, este último se centra en la traducción del habla de un formato verbal a otro textual, mientras que el primero sólo trata de identificar la voz de un usuario concreto.
Las aplicaciones de deep learning del mundo real están a nuestro alrededor y están tan bien integradas en productos y servicios que los usuarios no son conscientes del complejo procesamiento de datos que tiene lugar en segundo plano. Algunos de estos ejemplos incluyen:
Muchas organizaciones incorporan tecnología de deep learning en sus procesos del servicio de atención al cliente. Los chatbots se utilizan a menudo en diversas aplicaciones, servicios y portales de atención al cliente. Los chatbots tradicionales utilizan el lenguaje natural e incluso el reconocimiento visual, que se encuentra comúnmente en los menús de los centros de llamadas. Sin embargo, las soluciones de chatbots más sofisticadas intentan determinar, a través del aprendizaje, si hay múltiples respuestas a preguntas ambiguas en tiempo real. En función de las respuestas que recibe, el chatbot intenta responder a estas preguntas directamente o enruta la conversación a un usuario humano.

Asistentes virtuales como Siri de Apple, Alexa de Amazon o Google Assistant amplían la idea de un chatbot al permitir la funcionalidad de reconocimiento de voz. Esto crea un nuevo método para involucrar a los usuarios de una manera personalizada.
Las instituciones financieras utilizan con regularidad el análisis predictivo para impulsar la negociación algorítmica de acciones, evaluar los riesgos comerciales para la aprobación de préstamos, detectar fraudes y ayudar a administrar las carteras de crédito e inversión de los clientes.

El sector sanitario se ha beneficiado enormemente de las capacidades de deep learning desde la digitalización de los historiales y las imágenes de los hospitales. Las aplicaciones de reconocimiento de imágenes pueden ayudar a los especialistas en imágenes médicas y radiólogos, ayudándoles a analizar y evaluar más imágenes en menos tiempo.

Los algoritmos de deep learning pueden analizar y aprender de los datos de transacción para identificar patrones peligrosos que indiquen posibles actividades fraudulentas o delictivas. El reconocimiento de voz, la visión artificial y otras aplicaciones de deep learning pueden mejorar la eficiencia y la eficacia del análisis de investigación mediante la extracción de patrones y pruebas de grabaciones de sonido y vídeo, imágenes y documentos. Esta capacidad ayuda a las autoridades a analizar grandes cantidades de datos de forma más rápida y precisa.
IBM watsonx es una cartera de herramientas, aplicaciones y soluciones listas para el negocio, diseñadas para reducir los costos y los obstáculos de la adopción de la IA, al tiempo que optimiza los resultados y el uso responsable de la IA.
IBM watsonx Assistant es el chatbot IA para empresas. Esta tecnología de inteligencia artificial empresarial permite a los usuarios crear soluciones de IA conversacional.
Cree, ejecute y gestione modelos de IA. Prepare datos y construya modelos en cualquier nube utilizando código abierto o modelado visual. Pronostique y optimice sus resultados. 
Aprenda los conceptos fundamentales para la IA y la IA generativa, incluyendo ingeniería rápida, modelos de lenguaje extensos y los mejores proyectos de código abierto.
Explore esta rama del machine learning que se entrena con grandes cantidades de datos y se ocupa de las unidades computacionales que trabajan en tándem para realizar predicciones.
Explore los fundamentos de la arquitectura del machine learning y deep learning y descubra sus aplicaciones y beneficios asociados. 
Elegir el marco de deep learning adecuado en función de su carga de trabajo individual es un primer paso esencial en el deep learning.
Entrene, valide, ajuste e implemente IA generativa, modelos fundacionales y capacidades de machine learning con IBM watsonx.ai, un estudio empresarial de próxima generación para constructores de IA. Cree aplicaciones de IA en menos tiempo y con menos datos.
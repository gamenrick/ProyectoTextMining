Deep Learning: qué es y por qué va a ser una tecnología clave en el futuro de la inteligencia artificial
Autor desconocido
xataka.com

Raúl Arrabales
Desde los años 50 del siglo pasado y hasta hace muy pocos años el terreno habitual de la Inteligencia Artificial (IA) avanzada era mayoritariamente el laboratorio de investigación y la ciencia ficción. A excepción de casos contados, la práctica totalidad de sistemas con una inteligencia similar a la humana han aparecido en películas futurísticas u obras como las de Isaac Asimov. Sin embargo, este panorama está cambiando radicalmente en los últimos años. 
El gran impulso tecnológico al que solemos referirnos bajo el término Big Data ha revolucionado el entorno empresarial. Las organizaciones sometidas a la necesidad de la transformación digital se han convertido en criaturas sedientas de cantidades ingentes de datos; y por primera vez en la historia de la IA existe una demanda generalizada de sistemas con una inteligencia avanzada, equivalente a la de un humano, que sean capaces de procesar esos datos. Esto está ocurriendo en prácticamente todos los sectores, pues es rara la actividad empresarial o de la administración pública que no se pueda beneficiar de un análisis inteligente y automatizado de los datos. 
Vivimos un momento histórico, no porque las organizaciones quieran incorporar algo radicalmente nuevo, sino porque ahora son conscientes de que existe tecnología capaz de procesar todos los datos de los que disponen, hacerlo en escalas de tiempo inferiores a las humanas e incluso proporcionar la inteligencia necesaria.  
Podríamos decir que el Big Data ha sido simplemente la primera ola y el gran tsunami está a punto de llegar. Las nuevas arquitecturas Big Data han aparecido de la mano de las grandes compañías de Internet, organizaciones nativas digitales y completamente conectadas desde su concepción. En la actualidad estamos viendo como el Big Data prolifera rápidamente para abarcar todas las organizaciones y todos los sectores, pues en un ecosistema digital y global las compañías que no son nativas digitales también necesitan convertirse en devoradoras de datos. 
Una de las claves de la IA avanzada está en el aprendizaje. Es cada vez más habitual que les pidamos a las máquinas que aprendan por sí solas. No podemos permitirnos el lujo de pre-programar reglas para lidiar con las infinitas combinaciones de datos de entrada y situaciones que aparecen en el mundo real. 
En vez de hacer eso, necesitamos que las máquinas sean capaces de auto-programarse, en otras palabras, queremos máquinas que aprendan de su propia experiencia. La disciplina del Aprendizaje Automático (Machine Learning) se ocupa de este reto y gracias a la tormenta perfecta en la que nos acabamos de adentrar todos los gigantes de Internet han entrado de lleno en el mundo del aprendizaje automático, ofreciendo servicios en la nube para construir aplicaciones que aprenden a partir de los datos que ingieren. 
Hoy en día el aprendizaje automático está más que nunca al alcance de cualquier programador. Para experimentar con estos servicios tenemos plataformas como IBM Watson Developer Cloud, Amazon Machine Learning, Azure Machine Learning, TensorFlow o BigML. 
Entender los algoritmos de aprendizaje es fácil si nos fijamos en cómo aprendemos nosotros mismos desde niños. El aprendizaje por refuerzo engloba un grupo de técnicas de aprendizaje automático que a menudo usamos en los sistemas artificiales. En estos sistemas, al igual que en los niños, las conductas que se premian tienden a aumentar su probabilidad de ocurrencia, mientras que las conductas que se castigan tienden a desaparecer. 
Este tipo de enfoques se denominan aprendizaje supervisado, pues requiere de la intervención de los humanos para indicar qué está bien y qué está mal (es decir, para proporcional el refuerzo). En muchas otras aplicaciones de la computación cognitiva los humanos, aparte del refuerzo, también proporcionan parte de la semántica necesaria para que los algoritmos aprendan. Por ejemplo, en el caso de un software que debe aprender a diferenciar los diferentes tipos de documentos que recibe una oficina, son los humanos los que inicialmente han de etiquetar un conjunto significativo de ejemplos para que posteriormente la máquina pueda aprender. 
Es decir, los humanos son lo que inicialmente saben realmente si un documento es una queja, una instancia, una reclamación, una solicitud de registro, una petición de cambio, etc. Una vez que los algoritmos cuentan con un conjunto de entrenamiento proporcionado por los humanos, entonces son capaces de generalizar y empezar a clasificar documentos de forma automática sin intervención humana. 
En la actualidad son estas restricciones o limitaciones de entrenamiento de los algoritmos las que en buena medida limitan su potencia, pues se requieren buenos conjuntos de datos de entrenamiento (a menudo, etiquetados de forma manual por humanos) para que los algoritmos aprendan de forma efectiva. En el ámbito de la visión artificial, para que los algoritmos aprendan a detectar objetos en las imágenes de forma automática han de entrenarse previamente con un buen conjunto de imágenes etiquetadas, como por ejemplo Microsoft COCO. 
Posiblemente el futuro del aprendizaje automático pase por un giro hacia el aprendizaje no supervisado. En este paradigma los algoritmos son capaces de aprender sin intervención humana previa, sacando ellos mismos las conclusiones acerca de la semántica embebida en los datos. Ya existen compañías que se centran completamente en enfoques de aprendizaje automático no supervisado, como Loop AI Labs, cuya plataforma cognitiva es capaz de procesar millones de documentos no estructurados y construir de forma autónoma representaciones estructuradas. 
La disciplina del aprendizaje automático está en plena ebullición gracias a su aplicación en el mundo del Big Data y el IoT. No dejan de aparecer avances y mejoras de los algoritmos más tradicionales, desde los conjuntos de clasificadores (ensemble learning) hasta el Deep Learning, que está muy de moda en la actualidad por sus capacidad de acercarse cada vez más a la potencia perceptiva humana. 
En el enfoque Deep Learning se usan estructuras lógicas que se asemejan en mayor medida a la organización del sistema nervioso de los mamíferos, teniendo capas de unidades de proceso (neuronas artificiales) que se especializan en detectar determinadas características existentes en los objetos percibidos. La visión artificial es una de las áreas donde el Deep Learning proporciona una mejora considerable en comparación con algoritmos más tradicionales. Existen varios entornos y bibliotecas de código de Deep Learning que se ejecutan en las potentes GPUs modernas tipo CUDA, como por ejemplo NVIDIA cuDNN.
El Deep Learning representa un acercamiento más íntimo al modo de funcionamiento del sistema nervioso humano. Nuestro encéfalo tiene una microarquitectura de gran complejidad, en la que se han descubierto núcleos y áreas diferenciados cuyas redes de neuronas están especializadas para realizar tareas específicas. 
Gracias a la neurociencia, el estudio de casos clínicos de daño cerebral sobrevenido y los avances en diagnóstico por imagen sabemos por ejemplo que hay centros específicos del lenguaje (como las áreas de Broca o Wernicke), o que existen redes especializadas en detectar diferentes aspectos de la visión, como los bordes, la inclinación de las líneas, la simetría e incluso áreas íntimamente relacionadas con el reconocimiento de rostros y la expresión emocional de los mismos (el giro fusiforme en colaboración con la amígdala). 
Los modelos computacionales de Deep Learning imitan estas características arquitecturales del sistema nervioso, permitiendo que dentro del sistema global haya redes de unidades de proceso que se especialicen en la detección de determinadas características ocultas en los datos. Este enfoque ha permitido mejores resultados en tareas de percepción computacional, si las comparamos con las redes monolíticas de neuronas artificiales.
Hasta ahora hemos visto que la computación cognitiva se basa en la integración de procesos psicológicos típicamente humanos como el aprendizaje o el lenguaje. En los próximos años veremos como los sistemas cognitivos artificiales se expanden en múltiples aplicaciones en el ecosistema digital. 
Además, veremos como el aprendizaje y el lenguaje empiezan a integrarse con más funciones psicológicas como la memoria semántica, el razonamiento, la atención, la motivación y la emoción, de forma que los sistemas artificiales vayan acercándose más y más al nivel humano de inteligencia, o quizás, como ya se adelantaba en ConsScale (una escala para medir el desarrollo cognitivo), las máquinas puedan alcanzar niveles superiores a los humanos.
Una vez que las empresas disponen de los datos y los sistemas capaces de procesarlos es el momento de entrar de lleno en la siguiente fase: la comprensión de los datos, la adquisición del conocimiento y la extracción del valor. A pequeña escala esto es algo que tradicionalmente hacemos los humanos, accedemos a los datos, los interpretamos usando nuestro cerebro y tomamos decisiones supuestamente inteligentes. Sin embargo, cuando hablamos de gigabytes, terabytes o incluso petabytes de información, junto con la necesidad de tomar decisiones en escalas temporales del orden de los milisegundos, los humanos estamos literalmente fuera de combate. 
No tenemos más remedio que recurrir a máquinas y además necesitamos que estas máquinas sean capaces de interpretar los datos, comprenderlos y sacar conclusiones de forma inteligente. En otras palabras, necesitamos sistemas cognitivos artificiales, cerebros hechos de hardware y software, capaces de tomar decisiones por nosotros, capaces de realizar millones de tareas diferentes que en el pasado sólo podían hacer los humanos. 
Hoy en día multitud de productos y servicios, así como las estrategias de marketing que los envuelven, dependen de que las máquinas realicen de forma automática tareas como leer páginas web (con una excelente comprensión lectora), reconocer los rostros que aparecen en las imágenes publicadas en redes sociales, comprender la emoción contenida en el tono de voz de una conversación telefónica, contestar a las preguntas de un cliente en un chat, entender la dinámica y los motivos de los movimientos geográficos de las personas, predecir el gasto energético de una fábrica, inferir qué películas o canciones gustarán más a cada persona, recomendar la dieta y el ejercicio más saludable para cada persona en función de su estado actual de salud y su genotipo, etc. 
Todas estas tareas tienen algo en común. Todas requieren percibir lo que pasa en el entorno mediante la adquisición de datos y todas requieren realizar un procesamiento de la información para interpretar la realidad y extraer el significado (de forma que posteriormente se pueda razonar sobre este significado y tomar decisiones para realizar acciones adaptativas).  Precisamente por esto se produce una fiebre de los datos en todas las industrias. Al igual que en la fiebre del oro, existe un gran valor oculto en los millones y millones de toneladas de datos que una organización puede recabar. 
El primer objetivo es por lo tanto llegar a poder manejar cantidades tan ingentes de datos. Una vez que las modernas arquitecturas Big Data permiten almacenar y procesar decenas o cientos de petabytes de datos, el reto pasa a las fases de adquisición de datos y de interpretación de los mismos para la extracción de conocimiento. 
El internet de las cosas (IoT) supone un gran avance en el reto de la adquisición de los datos, mientras que la computación cognitiva aporta la inteligencia necesaria para la extracción del conocimiento. El momento que vivimos en la actualidad es de gran relevancia para el desarrollo de los sistemas inteligentes, pues nos encontramos ante una “tormenta perfecta”, originada gracias a la convergencia de las tecnologías Cloud, Móvil, IoT, Big Data y Computación Cognitiva. Según las previsiones de IDC, las empresas invertirán más de 31.000 millones de dólares en sistemas cognitivos artificiales en 2019. Los principales sectores por inversión en sistemas cognitivos son banca, comercio (retail) y salud.  
Para entender la envergadura y las implicaciones de esta tormenta perfecta podemos pensar en estas tecnologías como partes integrantes de un súper organismo de gran complejidad. Esta nueva Internet Cognitiva y Ubicua cuenta con un sistema sensorial que no para de extenderse gracias a los miles de millones de sensores conectados y desplegados por todas partes. 
Basta con contar el número de líneas móviles existentes en la actualidad para darse cuenta de que el número supera ya con creces el de habitantes humanos del planeta Tierra. Además, cada dispositivo móvil cuenta con múltiples sensores. La nueva red devora incesantemente volúmenes ingentes de datos que le permiten obtener información sobre el mundo. Gracias a esto aparecen multitud de nuevas oportunidades de negocio, basadas en la disponibilidad y explotación de estas nuevas fuentes de datos. 
El auge de los sistemas M2M (Machine to Machine) en el marco del Internet de las Cosas ha promovido un crecimiento exponencial del intercambio de datos entre las propias máquinas. Hemos pasado de un modelo tradicional, en el que los sensores obtenían información que luego usaban los humanos a un modelo en el que las máquinas ganan autonomía, pues los datos de los sensores ya no los consumen directamente los humanos, sino que pasan a formar parte del sistema perceptivo de la Red. 
La nueva Internet es una red que necesita percibir el mundo. De igual forma que los humanos percibimos el mundo que nos rodea mediante nuestros sentidos, en el modelo IoT la red cuenta con un repertorio de sentidos muy superior al humano. Mientras que nosotros vemos, oímos y olemos lo que tenemos a nuestro alrededor, las nuevas redes de sensores pueden extenderse miles de kilómetros usando la nube para comunicarse y para almacenar datos y además pueden usar muchas más modalidades sensoriales. 
Los dispositivos conectados perciben el mundo en aspectos tan variados como la geolocalización de un móvil, el ritmo cardiaco de la persona que usa una pulsera conectada, la temperatura del motor de un coche conectado, la dinámica de gasto de combustible de un reactor de un avión, el nivel de PH de la tierra donde se cultiva una vid, la altitud de vuelo de un drone, la radiación infrarroja que emite el firme de una carretera o la señal electroencefalográfica de un usuario de una silla de ruedas. Gracias al IoT todos estos datos pasan a formar parte del aparato perceptivo de un sistema artificial. 
Una vez que los entornos IoT permiten obtener información en tiempo real de las más variadas fuentes tenemos el problema de la interpretación y la comprensión de los datos. Aquí es donde se hacen necesarios los enfoques de computación cognitiva, pues las máquinas han de ser capaces de dar sentido y extraer el significado que se oculta tras los trillones de bytes que se mueven por la red. 
Los datos de los sensores M2M son fuentes de datos relativamente modernas, pero existe además una gran cantidad de conocimiento disponible en la propia web y las redes sociales. Se trata de cantidades ingentes de texto, audio y vídeo que contienen mucha información de interés. 
Uno de los principales motivos del auge de la computación cognitiva es que para acceder de forma efectiva a todas estas fuentes de información necesitamos máquinas capaces de leer millones y millones de documentos por nosotros. En los últimos años, gracias a nuevos avances como Watson de IBM, se suele identificar la computación cognitiva con la capacidad de las máquinas de procesar el lenguaje natural (el lenguaje natural es el que usamos los humanos, como por ejemplo el español o el inglés, que es bastante más complejo y rico que los lenguajes y códigos que usan los ordenadores tradicionales). 
Los sistemas cognitivos como Watson se usan para aprovechar todo el conocimiento disponible en bibliotecas de documentos o la propia Internet. De esta forma, cualquier usuario que use un sistema cognitivo de este tipo estaría en condiciones de aprovechar todo el conocimiento disponible sobre un tema. La revolución de la computación cognitiva implica un cambio radical en la forma en que accedemos a la información. 
En el enfoque tradicional hacemos consultas a un buscador como Google y tenemos que leer los resultados más relevantes, mientras que con la ayuda de asistentes cognitivos nosotros simplemente hacemos la pregunta y la máquina se ocupa dar la respuesta en base a lo que ha aprendido en su lectura de millones y millones de documentos y sensores (es lo que se denomina sistemas Q&A – Question & Answering). 
En esta tormenta perfecta tecnológica que estamos viviendo el impacto va mucho más allá de las máquinas. Estamos viendo como nuestra forma de relacionarnos con otras personas ha cambiado radicalmente en pocos años debido a la invasión de los dispositivos móviles. Con la llegada de tecnologías como el Deep Learning y la Computación Cognitiva nuestra forma de aprender, de relacionarnos y de entender el mundo va a cambiar también de forma radical. 
La inteligencia se produce cada vez de forma más distribuida, para dar solución a nuestros problemas ahora podemos preguntar directamente a nuestras máquinas y esperar una respuesta cada vez más inteligente. La gran responsabilidad que nos queda a los humanos es hacer las preguntas adecuadas.
Fotos | iStock | Pixabay
Los mejores comentarios:
Ver 8 comentarios
En Xataka hablamos de...
Ver más temas

Webedia

 Tecnología 
 Videojuegos 
 Entretenimiento 
 Gastronomía 
 Estilo de vida 

             Ediciones Internacionales
           
Destacamos

Ver más temas


Suscribir
Más sitios que te gustarán
Reciente
Ver más artículos
 Xataka
     TV

Ver más vídeos
Gemini (modelo de lenguaje) - Wikipedia, la enciclopedia libre
Autor desconocido
wikipedia.org

Gemini es un modelo de lenguaje grande multimodal desarrollado por Google DeepMind, que sirve como sucesor de LaMDA y PaLM. Comprende los modelos de Gemini Nano, Gemini Pro y Gemini Ultra. Fue anunciado el 6 de diciembre de 2023, posicionándose como competidor de GPT-4 de  OpenAI.

En la conferencia principal de Google I/O, celebrada el 10 de mayo de 2023, Google presentó Gemini, un avanzado modelo de lenguaje grande (LLM) creado por su subsidiaria, Google DeepMind. Este nuevo modelo se introduce como una versión más avanzada que PaLM 2, otro desarrollo destacado en el mismo evento.[1]​ Sundar Pichai, CEO de Google, señaló que Gemini todavía está en fases iniciales de desarrollo. Lo que distingue a Gemini de otros LLMs es su diseño único, que no se basa exclusivamente en texto, sino que es multimodal, permitiendo el procesamiento de varios tipos de datos.[2]​ Este proyecto es fruto de la colaboración entre DeepMind y Google Brain, que recientemente se unieron bajo el nombre Google DeepMind.

En una entrevista con Wired, Demis Hassabis, CEO de DeepMind, destacó las impresionantes capacidades de Gemini. Hassabis expresó su convicción de que Gemini podría superar a ChatGPT de OpenAI, que opera sobre GPT-4, en un contexto donde Google ha competido intensamente con desarrollos como LaMDA y Bard. También resaltó el éxito del programa AlphaGo de DeepMind, conocido por haber vencido al campeón de Go, Lee Sedol, en 2016. Hassabis mencionó que Gemini integra la potencia de AlphaGo con otras tecnologías LLM de Google y DeepMind.[3]​

En agosto de 2023, The Information publicó un informe que describía la hoja de ruta de Google para Gemini y revelaba que la compañía tenía como objetivo una fecha de lanzamiento para finales de 2023. Según el informe, Google esperaba superar a OpenAI y otros competidores combinando capacidades de texto conversacional presentes en la mayoría de los LLM con generación de imágenes impulsada por inteligencia artificial, lo que le permitiría crear imágenes contextuales y adaptarse a una gama más amplia de casos de uso. Al igual que Bard, el cofundador de Google, Sergey Brin, fue convocado desde su semi jubilación para ayudar en el desarrollo de Gemini, junto con cientos de otros ingenieros de Google Brain y DeepMind.   Debido a que Gemini estaba siendo entrenado en transcripciones de videos de YouTube, también se contrató a abogados para filtrar cualquier material potencialmente protegido por derechos de autor. Dylan Patel y Daniel Nishball de la firma de investigación Semi Analysis escribieron una publicación en un blog declarando que el lanzamiento de Gemini "devoraría el mundo" y superaría a GPT-4, lo que llevó al CEO de OpenAI, Sam Altman, a ridiculizar al dúo en X (anteriormente Twitter). Elon Musk, cofundador de OpenAI, intervino y preguntó: "¿Están equivocados los números?" 

Con la noticia del inminente lanzamiento de Gemini, OpenAI aceleró su trabajo para integrar GPT-4 con características multimodales similares a las de Gemini. The Information informó en septiembre que a varias empresas se les había concedido acceso anticipado a "una versión temprana" del LLM, que Google pretendía poner a disposición de los clientes a través del servicio Vertex AI de Google Cloud. La publicación también afirmó que Google estaba armando a Gemini para competir tanto con GPT-4 como con GitHub Copilot de Microsoft.  

El 6 de diciembre de 2023, Pichai y Hassabis anunciaron Gemini. [4]​[5]​

Una versión inicial de Gemini comenzó a desplegarse el mismo día dentro del chatbot Bard de Google para la configuración en inglés. Está disponible en más de 170 países y territorios. Google afirmó que Gemini se pondrá a disposición de los desarrolladores a través de la API de Google Cloud a partir del 13 de diciembre de 2023. Una versión más compacta del modelo impulsará las respuestas de mensajería sugeridas desde el teclado de los teléfonos inteligentes Pixel 8. Gemini se introducirá en otros productos de Google, como la búsqueda generativa, los anuncios y Chrome, en los "próximos meses", según la compañía. La versión Gemini más potente de todas debutará en 2024, a la espera de "exhaustivas comprobaciones de confianza y seguridad", afirma Google.[6]​

Se dice que Gemini Ultra superó a GPT-4, Claude 2 de Anthropic, Inflection-2 de Inflection AI, LLaMA 2 de Meta y Grok 1 de xAI en una variedad de pruebas de referencia de la industria,[7]​ mientras que se dice que Gemini Pro superó a GPT-3.5. Gemini Ultra también fue el primer modelo de lenguaje que superó a los expertos humanos en la prueba de Comprensión de Lenguaje Multitarea Masiva (MMLU) de 57 temas, obteniendo una puntuación del 90%.[8]​ Gemini Pro estará disponible para los clientes de Google Cloud en AI Studio y Vertex AI el 13 de diciembre de 2023, mientras que Gemini Nano estará disponible también para los desarrolladores de Android. 

De acuerdo con la Orden Ejecutiva 14110 firmada por el presidente Joe Biden en octubre de 2023, Google declaró que compartiría los resultados de las pruebas de Gemini Ultra con el gobierno federal de los Estados Unidos. Del mismo modo, la empresa estaba en conversaciones con el gobierno del Reino Unido para cumplir con los principios establecidos en la Cumbre de Seguridad de la IA en Bletchley Park en noviembre de 2023.

Los usuarios de la Unión Europea y el Reino Unido no podrán utilizar Gemini en el momento de su lanzamiento por motivos de protección de datos.[9]​

El 1 de febrero de 2023, se anunció la integración de Gemini Pro a Bard para más de 40 idiomas y en más de 230 países.[10]​

Gemini se basa en transformadores que solo decodifican, pero con algunos cambios para que funcionen mejor en las TPUs (unidades de procesamiento tensorial). Pueden manejar hasta 32.768 tokens de contexto, usando una técnica llamada atención multi-consulta. 

Hay dos versiones pequeñas de Gemini: Nano-1 (1.8 mil millones de parámetros) y Nano-2 (3.25 mil millones de parámetros), que se obtienen a partir de modelos Gemini más grandes, pensados para dispositivos de borde como los teléfonos inteligentes.

Gemini es multimodal, lo que quiere decir que puede procesar diferentes tipos de entrada. No importa el orden ni la combinación de los modos, lo que permite tener una conversación multimodal. Las imágenes pueden tener distintas resoluciones, y los vídeos se tratan como una serie de imágenes. El audio se muestrea a 16 kHz y luego se transforma en una secuencia de tokens por el Modelo Universal de Habla.

Gemini se entrena con un conjunto de datos que también es multimodal y multilingüe, formado por documentos web, libros y código, e incluye datos de imagen, audio y vídeo.[11]​

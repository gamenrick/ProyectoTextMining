Deep learning: qué es y sus aplicaciones
Autor desconocido
redhat.com

Le recomendaremos recursos que le pueden ser útiles mientras navega en redhat.com. Por ahora, vea estos.
   
Conozca la manera en que Red Hat® OpenShift® AI agiliza todo el ciclo de vida de los modelos y las aplicaciones de inteligencia artificial y machine learning con herramientas integradas, flujos de trabajo repetibles, opciones flexibles de implementación y un ecosistema de partners confiable.

El aprendizaje profundo es una técnica de la inteligencia artificial (IA) que consiste en enseñar a las computadoras a procesar los datos mediante algoritmos inspirados en el cerebro humano. 
En el deep learning (aprendizaje profundo), se utiliza la inteligencia artificial y el aprendizaje automático (IA/ML) para que los analistas de datos puedan recopilar, analizar e interpretar enormes cantidades de datos. Este proceso, también conocido como aprendizaje neuronal profundo o redes neuronales profundas, consiste en que las computadoras adquieran el aprendizaje a través de la observación, de manera similar a las personas. 
El cerebro humano tiene muchas neuronas interconectadas que actúan como mensajeras cuando se procesa la información (o los datos). Las neuronas utilizan impulsos eléctricos y señales químicas para comunicarse entre sí y transmitir la información entre las distintas áreas del cerebro. 
Las redes neuronales artificiales (ANN) constituyen la arquitectura subyacente del deep learning (aprendizaje profundo) y se basan en este fenómeno biológico, pero se forman con neuronas artificiales que se crean a partir de módulos de software denominados nodos. Los nodos utilizan cálculos matemáticos, a diferencia de las señales químicas que se usan en el cerebro, para comunicar y transmitir la información. Esta red neuronal simulada (SNN) procesa los datos agrupándolos en clústeres y realizando predicciones.
Piense en el deep learning (aprendizaje profundo) como una especie de diagrama de flujo que comienza con una capa de entrada y termina con una de salida. Entre ellas, se encuentran las "capas ocultas" que procesan la información, las cuales se encargan de ajustar y adaptar su comportamiento a medida que reciben datos nuevos. Los modelos de deep learning (aprendizaje profundo) pueden tener cientos de capas ocultas, y cada una de ellas cumple una función en el descubrimiento de las relaciones y los patrones dentro del conjunto de datos. 
La capa de entrada está compuesta por varios nodos: los datos se incorporan al modelo y se categorizan según corresponda antes de continuar con el proceso. La ruta de los datos a través de las capas se basa en los cálculos de cada nodo. Los datos pasan por cada una de las capas, y se recopilan conclusiones durante el proceso, las cuales acabarán por convertirse en la capa de salida o el análisis final.
Las aplicaciones que utilizan el deep learning (aprendizaje profundo) ya están integradas a la vida diaria y se utilizan en diferentes sectores. En la actualidad, muchas herramientas se basan en la inteligencia artificial generativa, la cual es posible gracias a esta técnica.
Sus casos prácticos evolucionan constantemente, pero podemos destacar tres de los más conocidos en la actualidad: la visión artificial, el reconocimiento de voz y el procesamiento del lenguaje natural (NLP). 
Estos son algunos ejemplos de los usos de los principios del deep learning (aprendizaje profundo) en los distintos sectores:
IBM y Red Hat trabajaron juntos para crear Red Hat® Ansible® Lightspeed with IBM watsonx Code Assistant, un servicio de inteligencia artificial generativa que permite a los desarrolladores generar contenido de Ansible con mayor eficiencia.
El deep learning (aprendizaje profundo) es una clase de aprendizaje automático especializado, y se diferencia por el tipo de datos con los que trabaja y los métodos que utiliza.
Los algoritmos clásicos del aprendizaje automático requieren cierto grado de intervención humana para procesar los conjuntos de datos antes de incorporarlos al modelo. Es decir que se definen y etiquetan funciones específicas a partir de los datos de entrada y, luego, se organizan en tablas antes de incorporarlas al modelo de aprendizaje automático. Por el contrario, los algoritmos del deep learning (aprendizaje profundo) no necesitan ese procesamiento previo y pueden comprender los datos no estructurados, como los documentos de texto, las imágenes de datos de píxel o los archivos de datos de audio. 
Es preferible utilizar el deep learning (aprendizaje profundo), en lugar del automático clásico, en los casos en los que haya una enorme cantidad de datos, no se disponga de suficiente conocimiento previo sobre el tema o se trate de una tarea compleja que lleve mucho tiempo.
Sabemos que el deep learning (aprendizaje profundo) utiliza una estructura de nodos que se comunican entre sí mediante una red neuronal artificial. Para crear una ANN, se deben incorporar cálculos y parámetros al modelo junto con los datos, y se deben tomar precauciones para garantizar que los cálculos tengan en cuenta el sesgo y las varianzas. 
En el contexto del aprendizaje automático, el sesgo hace referencia a la medida en la que el modelo realiza suposiciones o generalizaciones sobre los datos para facilitar el aprendizaje de la función objetivo. Si el sesgo es alto, el modelo es demasiado simple y crea atajos al procesar la información. 
La varianza es la medida de la distancia que separa cada dato de la media, o la medida estadística de la dispersión entre las cifras de un conjunto de datos. Contrariamente a lo que sucede con el sesgo, la varianza se refiere a la sensibilidad de un modelo frente a los datos de entrenamiento. La varianza (o sensibilidad) alta significa que el modelo presta demasiada atención a los detalles y pasa por alto los patrones subyacentes del conjunto de datos. 
En el aprendizaje supervisado, se habla de sobreajuste cuando la varianza es muy alta y el sesgo es muy bajo. Cuando el sesgo es alto y la varianza es baja, se denomina subajuste. Puede ser difícil llegar al ajuste ideal, conocido como el equilibrio entre el sesgo y la varianza. 
Los parámetros definen los límites, los cuales son fundamentales para comprender la enorme cantidad de datos que deben procesar los algoritmos del deep learning (aprendizaje profundo). Esto significa que, generalmente, se pueden corregir los sobreajustes y subajustes utilizando menos o más parámetros, según corresponda. 
Si un modelo de deep learning (aprendizaje profundo) se entrena con datos sesgados o datos que no brindan una representación precisa de la población, se puede obtener un resultado erróneo. Desafortunadamente, los prejuicios humanos suelen transferirse a la inteligencia artificial, lo cual implica el riesgo de que se creen algoritmos discriminatorios y resultados sesgados. 
A medida que las empresas aprovechan la IA para mejorar la productividad y el rendimiento, es fundamental que se implementen estrategias para minimizar el sesgo. El primer paso es la implementación de procesos inclusivos de diseño y una mayor consciencia de la representación de la diversidad en los datos recopilados. 
El término "caja negra" hace referencia a los programas de inteligencia artificial que ejecutan una tarea dentro de la red neuronal y no muestran su trabajo, lo cual implica que nadie puede explicar con exactitud cómo se generó un resultado en particular, ni siquiera los analistas de datos ni los ingenieros que crearon el algoritmo. La falta de interpretación de los modelos de caja negra puede generar consecuencias dañinas cuando se utilizan para la toma de decisiones que conllevan grandes riesgos, en especial en los sectores de atención de la salud, justicia penal y finanzas. 
Los modelos de deep learning (aprendizaje profundo) pueden llevar a cabo tareas informáticas más complejas sin intervención humana, pero esto implica que necesitan más potencia de procesamiento, una infraestructura adecuada y conjuntos más grandes de datos para el entrenamiento. El cloud computing permite que los equipos accedan simultáneamente a varios procesadores, como los clústeres de unidades gráficas de procesamiento (GPU) y de unidades centrales de procesamiento (CPU), lo cual crea el entorno ideal para que se ejecuten las operaciones matemáticas complejas.
Cuando los equipos de desarrollo diseñan, desarrollan y entrenan los modelos de deep learning (aprendizaje profundo) en la nube, pueden ajustar y distribuir las cargas de trabajo con velocidad y precisión y, al mismo tiempo, reducir los costos operativos. 
El uso de la nube implica nuevas opciones para implementar el aprendizaje automático en el extremo de la red. Mediante la creación de centros de edge computing conectados a los recursos de la nube pública, se puede obtener y analizar la información de inmediato para intervenir en operaciones que pueden variar desde la actualización del estado de la cadena de suministro hasta la prestación de información sobre los lugares de evacuación en caso de una catástrofe. 
Red Hat ofrece una base común para que sus equipos diseñen e implementen las aplicaciones de inteligencia artificial y los modelos de aprendizaje automático de forma transparente y regulada. 
Red Hat® OpenShift® AI es una plataforma que puede entrenar los modelos de inteligencia artificial con sus propios datos, distribuirlos y aplicar en ellos las técnicas de ajuste de instrucciones y perfeccionamiento para sus casos prácticos específicos.
Para las implementaciones de inteligencia artificial de gran tamaño, Red Hat OpenShift ofrece una plataforma de aplicaciones adaptable que es adecuada para las cargas de trabajo de inteligencia artificial, con acceso a los aceleradores de hardware conocidos.
Red Hat también utiliza sus propias herramientas de Red Hat OpenShift AI para mejorar la utilidad de otros sistemas de software open source, como Red Hat Ansible Lightspeed with IBM watsonx Code Assistant. Gracias a este servicio, los equipos de automatización pueden aprender el contenido de Ansible, crearlo y mantenerlo de manera más eficiente. Recibe instrucciones de los usuarios y, luego, interactúa con los modelos base de IBM watsonx para generar recomendaciones de código, que después se utilizarán para crear playbooks de Ansible.
Además, las integraciones de los partners de Red Hat abren paso a un ecosistema de herramientas confiables de inteligencia artificial diseñadas para funcionar en las plataformas open source.
Artículo
La inteligencia artificial generativa crea contenido nuevo a partir de los modelos de aprendizaje profundo que están entrenados con conjuntos grandes de datos.
Artículo
El aprendizaje automático es la técnica que consiste en entrenar a una computadora para que encuentre patrones, realice predicciones y aprenda de la experiencia sin una programación explícita.
Artículo
Se trata de modelos de aprendizaje automático que se entrenan previamente para llevar a cabo diversas tareas. 
Una plataforma de modelo base para desarrollar, probar y ejecutar modelos de lenguaje de gran tamaño (LLM) de Granite para aplicaciones empresariales.
Cartera de soluciones centradas en la inteligencia artificial que ofrece herramientas para entrenar, mejorar, distribuir, supervisar y gestionar los modelos y los experimentos con la tecnología de inteligencia artificial/machine learning en Red Hat OpenShift
Una plataforma de aplicaciones empresariales que ofrece servicios probados para lanzar aplicaciones al mercado en la infraestructura que tú escojas. 
Red Hat Ansible Lightspeed with IBM watsonx Code Assistant es un servicio de inteligencia artificial generativa diseñado por y para los automatizadores, los operadores y los desarrolladores de Ansible. 

                    ebook
                                                                            
Los aspectos principales en el diseño de un entorno de AI/ML listo para la producción

                    Informe de analistas
                                                                            
Impacto Económico Total™ (TEI) de Red Hat Hybrid Cloud Platform para MLOps

                    Webinar
                                                                            
Getting the most out of AI with open source and Kubernetes
CARTERA DE PRODUCTOS
Inteligencia artificial de Red Hat
EBOOK
Implemente inteligencia artificial/machine learning para potenciar su empresa
PUBLICACIÓN DE BLOG
La IA y el ML y su importancia para las empresas
PARTNERS
Descubre el ecosistema de partners de inteligencia artificial de Red Hat
CASO DE ÉXITO
El Banco Galicia agiliza la incorporación de clientes nuevos
Somos el proveedor líder a nivel mundial de soluciones empresariales de código abierto, incluyendo Linux, cloud, contenedores y Kubernetes. Ofrecemos soluciones reforzadas, las cuales permiten que las empresas trabajen en distintas plataformas y entornos con facilidad, desde el centro de datos principal hasta el extremo de la red.
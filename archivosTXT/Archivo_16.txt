Los modelos de lenguaje de gran tamaño o LLM: qué son y cómo funcionan?
Autor desconocido
redhat.com

Le recomendaremos recursos que le pueden ser útiles mientras navega en redhat.com. Por ahora, vea estos.
Un modelo de lenguaje de gran tamaño (LLM) es un tipo de modelo de inteligencia artificial que emplea técnicas de machine learning para comprender y generar lenguaje humano. Estos modelos pueden resultar muy valiosos para las empresas y las entidades que buscan automatizar y mejorar diversos aspectos de la comunicación y del procesamiento de datos. 
Los LLM utilizan modelos basados en redes neuronales y técnicas de procesamiento de lenguajes naturales (NLP) para procesar y estimar sus resultados. El NLP es un campo de la inteligencia artificial (IA) que se centra en lograr que las computadoras comprendan, interpreten y generen texto. Esto, a su vez, permite que los LLM realicen diversas tareas: analizar texto y sentimientos u opiniones, traducir idiomas y reconocer voces.
Conozca la inteligencia artificial de Red Hat
Los LLM utilizan un método denominado aprendizaje no supervisado para comprender el lenguaje. Este proceso consiste en proporcionar conjuntos de datos (cientos de miles de millones de palabras y frases) a un modelo de aprendizaje automático para que los estudie y aprenda mediante el ejemplo. Esta fase de preentrenamiento mediante aprendizaje no supervisado es fundamental para el desarrollo de los LLM, como GPT-3 (Transformador generativo entrenado previamente) y BERT (Representación de codificador bidireccional de transformadores). 
En otras palabras, aunque la computadora no reciba instrucciones explícitas de nuestra parte, puede obtener información a partir de los datos, establecer conexiones y "aprender" sobre el lenguaje. A medida que el modelo conoce los patrones a partir de los cuales se enlazan las palabras, puede hacer predicciones sobre el modo en que deben estructurarse las frases, en función de la probabilidad. Como resultado final, se obtiene un modelo capaz de captar las complejas relaciones entre las palabras y las frases. 
Los LLM requieren muchos recursos
Dado que deben calcular constantemente las probabilidades para hallar conexiones, los LLM utilizan una cantidad considerable de recursos informáticos. Uno de los recursos de los cuales obtienen potencia informática son las unidades de procesamiento gráfico (GPU). Las GPU son elementos especializados de hardware diseñados para gestionar tareas complejas de procesamiento paralelo, lo que hace que sean ideales para los modelos de aprendizaje automático y aprendizaje profundo que deben realizar muchos cálculos, como los LLM.
Los LLM y los transformadores
Las GPU también son esenciales para agilizar el entrenamiento y el funcionamiento de los transformadores: un tipo de arquitectura de software diseñada específicamente para realizar las tareas de NLP que implementan la mayoría de los LLM. Los transformadores son una parte fundamental de los modelos base de los LLM más conocidos, como ChatGPT y BERT.
Las arquitecturas de transformadores mejoran la utilidad de los modelos de aprendizaje automático, ya que captan eficazmente las relaciones y dependencias contextuales entre los elementos de una secuencia de datos, por ejemplo, las palabras de una oración. Para ello, emplean mecanismos de autoatención, también conocidos como parámetros, que permiten que el modelo evalúe la importancia de los distintos elementos de la secuencia, lo cual mejora su comprensión y rendimiento. Los parámetros definen los límites, los cuales son fundamentales para interpretar la enorme cantidad de datos que deben procesar los algoritmos del aprendizaje profundo.
En la arquitectura de los transformadores intervienen millones o miles de millones de parámetros, los cuales le permiten captar patrones y matices del lenguaje de gran complejidad. De hecho, el término "de gran tamaño" en "modelo de lenguaje de gran tamaño" se refiere a la enorme cantidad de parámetros necesarios para hacer funcionar un LLM.
Los LLM y el aprendizaje profundo
Los transformadores y los parámetros que guían el proceso de aprendizaje no supervisado de un LLM forman parte de una estructura más amplia denominada aprendizaje profundo. Se conoce como aprendizaje profundo a aquella técnica de inteligencia artificial que consiste en enseñar a las computadoras a procesar los datos mediante algoritmos inspirados en el cerebro humano. Este proceso, también conocido como aprendizaje neuronal profundo o redes neuronales profundas, permite que las computadoras adquieran el aprendizaje a través de la observación, de manera similar a las personas. 
El cerebro humano tiene muchas neuronas interconectadas que actúan como mensajeras cuando se procesa la información (o los datos). Las neuronas utilizan impulsos eléctricos y señales químicas para comunicarse entre sí y transmitir la información entre las distintas áreas del cerebro. 
Las redes neuronales artificiales (ANN) constituyen la arquitectura subyacente del aprendizaje profundo y se basan en este fenómeno biológico, pero se forman con neuronas artificiales creadas a partir de módulos de software denominados nodos. Los nodos utilizan cálculos matemáticos (en lugar de señales químicas como el cerebro) para comunicar y transmitir la información dentro del modelo.
Conozca más sobre los modelos de lenguaje de gran tamaño
Los LLM modernos pueden comprender y utilizar el lenguaje como nunca antes se había podido esperar de una computadora personal. Estos modelos de aprendizaje automático pueden generar texto, resumir contenido o reescribirlo, traducir, clasificar u ordenar algo por categorías, realizar análisis y mucho más. Todas estas funciones nos ofrecen un importante conjunto de herramientas que aumentan nuestra creatividad y productividad a la hora de resolver problemas complejos.
Algunos de los usos más comunes de los LLM en los entornos empresariales pueden ser:
Automatización y eficienciaLos LLM pueden complementar las tareas relacionadas con el lenguaje, como el servicio de soporte al cliente, el análisis de datos y la generación de contenido, o encargarse de ellas por completo. Al automatizar estas tareas, es posible reducir los costos operativos y destinar los recursos humanos a las tareas más estratégicas. 
Generación de informaciónLos LLM pueden revisar grandes cantidades de datos de texto con rapidez y extraer información de diversas fuentes, como las redes sociales, las reseñas y los artículos de investigación, para que las empresas comprendan mejor las tendencias del mercado y los comentarios de los clientes y tomen decisiones bien fundamentadas.
Mejora de la experiencia del clienteLos LLM permiten que las empresas ofrezcan contenido altamente personalizado a sus clientes, lo cual fomenta su participación y mejora su experiencia. Por ejemplo, pueden implementar un chatbot para ofrecer un servicio ininterrumpido de soporte al cliente, adaptar los mensajes de marketing a usuarios específicos o facilitar la traducción de idiomas y la comunicación intercultural. 
Desafíos y limitaciones de los LLM
Aunque el empleo de un LLM en un entorno empresarial ofrece muchas ventajas, también hay que tener en cuenta sus posibles limitaciones:
 
Los casos prácticos transformadores de inteligencia artificial/machine learning surgen en los sectores automotriz, financiero, de la salud y de las telecomunicaciones, entre otros. Nuestras plataformas open source y nuestro sólido ecosistema de partners ofrecen soluciones integrales para crear, implementar y gestionar los modelos de deep learning y machine learning para las aplicaciones que trabajan con inteligencia artificial.
Red Hat® OpenShift® es una plataforma líder de desarrollo de contenedores para entornos híbridos y multicloud que posibilita la colaboración entre los analistas de datos y los desarrolladores de software. Agiliza la implementación de las aplicaciones inteligentes en los entornos de nube híbrida, desde los centros de datos hasta el extremo de la red y varias nubes.
Con Red Hat OpenShift AI, las empresas tienen acceso a los recursos para agilizar el desarrollo, el entrenamiento, las pruebas y la implementación de los modelos de machine learning organizados en contenedores, sin tener que diseñar ni instalar la infraestructura de Kubernetes. Los usuarios pueden ajustar sus sistemas con más confianza para entrenar los modelos base con las funciones de aceleración de la GPU propias de OpenShift, ya sea en las instalaciones o mediante un servicio de nube. 
Red Hat Ansible® Lightspeed with IBM watsonx Code Assistant es un servicio de inteligencia artificial generativa que permite a los desarrolladores generar contenido de Ansible con mayor eficiencia. Lee el texto sencillo que los usuarios escriben en inglés y, luego, interactúa con los modelos base IBM watsonx para generar recomendaciones de código para las tareas de automatización, que después se utilizarán para crear playbooks de Ansible. Implemente Ansible Lightspeed en Red Hat OpenShift para simplificar las tareas difíciles en Kubernetes mediante la automatización y la organización inteligentes. 
Ebook: Inteligencia artificial/machine learning en Red Hat OpenShift
Aquí encuentras la información más reciente sobre nuestros clientes, partners y comunidades open source.
E-book

Introducción a la inteligencia artificial para las empresas: guía para principiantes

Hoja de datos

Plataforma de modelos base para la inteligencia artificial generativa

Publicación en blog

Mano a mano con Fran Heeran, vicepresidente de telecomunicaciones globales de Red Hat

Publicación en blog

Manual para ejecutivos: cómo entender el panorama de la IA

Somos el proveedor líder a nivel mundial de soluciones empresariales de código abierto, incluyendo Linux, cloud, contenedores y Kubernetes. Ofrecemos soluciones reforzadas, las cuales permiten que las empresas trabajen en distintas plataformas y entornos con facilidad, desde el centro de datos principal hasta el extremo de la red.
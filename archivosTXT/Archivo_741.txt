¿Qué es Aprendizaje profundo (deep learning)? - Definición en Computer Weekly
Autor desconocido
computerweekly.com

¿Qué es el aprendizaje profundo?
El aprendizaje profundo (Deep Learning) es un tipo de aprendizaje automático (machine learning, ML) e inteligencia artificial (IA) que imita la forma en que los humanos obtienen ciertos tipos de conocimiento. 
El aprendizaje profundo es un elemento importante de la ciencia de datos, que incluye estadísticas y modelos predictivos. Es extremadamente beneficioso para los científicos de datos que tienen la tarea de recopilar, analizar e interpretar grandes cantidades de datos; el aprendizaje profundo hace que este proceso sea más rápido y sencillo.
En su forma más simple, el aprendizaje profundo se puede considerar como una forma de automatizar el análisis predictivo. Si bien los algoritmos de aprendizaje automático tradicionales son lineales, los algoritmos de aprendizaje profundo se apilan en una jerarquía de complejidad y abstracción cada vez mayores.
Para comprender el aprendizaje profundo, imagine a un niño pequeño cuya primera palabra es perro. El niño pequeño aprende qué es —y qué no es— un perro señalando objetos y diciendo la palabra perro. El padre dice: "Sí, ese es un perro" o "No, ese no es un perro". A medida que el niño pequeño continúa señalando objetos, se vuelve más consciente de las características que poseen todos los perros. Lo que hace el niño, sin saberlo, es aclarar una abstracción compleja —el concepto de perro— mediante la construcción de una jerarquía en la que cada nivel de abstracción se crea con el conocimiento obtenido de la capa anterior de la jerarquía.
Los programas informáticos que utilizan el aprendizaje profundo pasan por el mismo proceso que el niño pequeño que aprende a identificar al perro. Cada algoritmo de la jerarquía aplica una transformación no lineal a su entrada y usa lo que aprende para crear un modelo estadístico como salida. Las iteraciones continúan hasta que la salida alcanza un nivel aceptable de precisión. La cantidad de capas de procesamiento a través de las cuales deben pasar los datos es lo que inspiró la etiqueta profundo.
En el aprendizaje automático tradicional, el proceso de aprendizaje está supervisado y el programador tiene que ser extremadamente específico al decirle a la computadora qué tipo de cosas debe buscar para decidir si una imagen contiene un perro o no. Este es un proceso laborioso llamado extracción de características, y la tasa de éxito de la computadora depende completamente de la capacidad del programador para definir con precisión un conjunto de características para perro. La ventaja del aprendizaje profundo es que el programa crea el conjunto de funciones por sí mismo sin supervisión. El aprendizaje no supervisado no solo es más rápido, sino que suele ser más preciso.
Inicialmente, el programa informático podría contar con datos de entrenamiento — un conjunto de imágenes para las que un humano ha etiquetado cada imagen como perro o no perro con metaetiquetas. El programa utiliza la información que recibe de los datos de entrenamiento para crear un conjunto de características para perro y construir un modelo predictivo. En este caso, el modelo que crea primero la computadora podría predecir que cualquier cosa en una imagen que tenga cuatro patas y una cola debería etiquetarse como perro. Por supuesto, el programa no conoce las etiquetas de cuatro patas o cola. Simplemente buscará patrones de píxeles en los datos digitales. Con cada iteración, el modelo predictivo se vuelve más complejo y preciso.
A diferencia del niño pequeño, que tardará semanas o incluso meses en comprender el concepto de perro, a un programa informático que utiliza algoritmos de aprendizaje profundo se le puede mostrar un conjunto de entrenamiento y clasificar millones de imágenes, identificando con precisión qué imágenes tienen perros en unos pocos minutos.
Para lograr un nivel aceptable de precisión, los programas de aprendizaje profundo requieren acceso a inmensas cantidades de datos de entrenamiento y potencia de procesamiento, ninguno de los cuales estaba fácilmente disponible para los programadores hasta la era del big data y la computación en la nube. Debido a que la programación de aprendizaje profundo puede crear modelos estadísticos complejos directamente a partir de su propia salida iterativa, puede crear modelos predictivos precisos a partir de grandes cantidades de datos no estructurados y sin etiquetar. Esto es importante ya que el internet de las cosas (IoT) continúa volviéndose más omnipresente porque la mayoría de los datos que crean los humanos y las máquinas no están estructurados y no están etiquetados.
Se pueden utilizar varios métodos para crear modelos sólidos de aprendizaje profundo. Estas técnicas incluyen la disminución de la tasa de aprendizaje, el aprendizaje por transferencia, la formación desde cero y la deserción.
Decadencia de la tasa de aprendizaje. La tasa de aprendizaje es un hiperparámetro —un factor que define el sistema o establece las condiciones para su funcionamiento antes del proceso de aprendizaje— que controla cuánto cambio experimenta el modelo en respuesta al error estimado cada vez que se modifican los pesos del modelo. Las tasas de aprendizaje que son demasiado altas pueden resultar en procesos de entrenamiento inestables o en el aprendizaje de un conjunto de pesos subóptimo. Las tasas de aprendizaje que son demasiado pequeñas pueden producir un proceso de capacitación prolongado que tiene el potencial de atascarse.
El método de disminución de la tasa de aprendizaje —también llamado recocido de la tasa de aprendizaje o tasas de aprendizaje adaptativas— es el proceso de adaptar la tasa de aprendizaje para aumentar el rendimiento y reducir el tiempo de capacitación. Las adaptaciones más fáciles y comunes de la tasa de aprendizaje durante el entrenamiento incluyen técnicas para reducir la tasa de aprendizaje con el tiempo.
Transferir aprendizaje. Este proceso implica perfeccionar un modelo previamente entrenado; requiere una interfaz para el interior de una red preexistente. Primero, los usuarios alimentan la red existente con nuevos datos que contienen clasificaciones previamente desconocidas. Una vez que se realizan los ajustes a la red, se pueden realizar nuevas tareas con capacidades de categorización más específicas. Este método tiene la ventaja de requerir muchos menos datos que otros, reduciendo así el tiempo de cálculo a minutos u horas.
Entrenar desde cero. Este método requiere que un desarrollador recopile un gran conjunto de datos etiquetados y configure una arquitectura de red que pueda aprender las características y el modelo. Esta técnica es especialmente útil para aplicaciones nuevas, así como para aplicaciones con una gran cantidad de categorías de salida. Sin embargo, en general, es un enfoque menos común, ya que requiere cantidades excesivas de datos, lo que hace que la capacitación demore días o semanas.
Abandonar. Este método intenta resolver el problema del sobreajuste en redes con grandes cantidades de parámetros al soltar aleatoriamente unidades y sus conexiones de la red neuronal durante el entrenamiento. Se ha comprobado que el método de abandono puede mejorar el rendimiento de las redes neuronales en tareas de aprendizaje supervisado en áreas como reconocimiento de voz, clasificación de documentos y biología computacional.
Un tipo de algoritmo de aprendizaje automático avanzado, conocido como red neuronal artificial, sustenta la mayoría de los modelos de aprendizaje profundo. Como resultado, el aprendizaje profundo a veces puede denominarse aprendizaje neuronal profundo o redes neuronales profundas.
Las redes neuronales vienen en varias formas diferentes, incluidas las redes neuronales recurrentes, las redes neuronales convolucionales, las redes neuronales artificiales y las redes neuronales de retroalimentación, y cada una tiene beneficios para casos de uso específicos. Sin embargo, todos funcionan de manera algo similar —introduciendo datos y dejando que el modelo averigüe por sí mismo si ha tomado la interpretación o decisión correcta sobre un elemento de datos dado.
Las redes neuronales implican un proceso de prueba y error, por lo que necesitan cantidades masivas de datos para entrenar. No es una coincidencia que las redes neuronales se hicieran populares solo después de que la mayoría de las empresas adoptaron el análisis de big data y acumularon grandes cantidades de datos. Debido a que las primeras iteraciones del modelo implican suposiciones un tanto fundamentadas sobre el contenido de una imagen o partes del discurso, los datos utilizados durante la etapa de entrenamiento deben etiquetarse para que el modelo pueda ver si su suposición fue precisa. Esto significa que, aunque muchas empresas que utilizan big data tienen grandes cantidades de datos, los datos no estructurados son menos útiles. Los datos no estructurados solo pueden ser analizados por un modelo de aprendizaje profundo una vez que han sido entrenados y alcanzan un nivel aceptable de precisión, pero los modelos de aprendizaje profundo no pueden entrenarse con datos no estructurados.
Debido a que los modelos de aprendizaje profundo procesan la información de manera similar al cerebro humano, se pueden aplicar a muchas tareas que realizan las personas. El aprendizaje profundo se utiliza actualmente en las herramientas de reconocimiento de imágenes más comunes, el procesamiento del lenguaje natural (NLP) y el software de reconocimiento de voz. Estas herramientas están empezando a aparecer en aplicaciones tan diversas como los coches autónomos y los servicios de traducción de idiomas.
Los casos de uso actuales para el aprendizaje profundo incluyen todo tipo de aplicaciones de análisis de big data, especialmente aquellas enfocadas en NLP, traducción de idiomas, diagnóstico médico, señales de negociación del mercado de valores, seguridad de red y reconocimiento de imágenes.
Los campos específicos en los que se utiliza actualmente el aprendizaje profundo incluyen los siguientes:
La mayor limitación de los modelos de aprendizaje profundo es que aprenden a través de observaciones. Esto significa que solo saben lo que había en los datos sobre los que se entrenaron. Si un usuario tiene una pequeña cantidad de datos o proviene de una fuente específica que no es necesariamente representativa del área funcional más amplia, los modelos no aprenderán de una manera generalizable.
El tema de los sesgos también es un problema importante para los modelos de aprendizaje profundo. Si un modelo se entrena con datos que contienen sesgos, el modelo reproducirá esos sesgos en sus predicciones. Este ha sido un problema molesto para los programadores de aprendizaje profundo porque los modelos aprenden a diferenciar en función de variaciones sutiles en los elementos de datos. A menudo, los factores que determina que son importantes no se le aclaran explícitamente al programador. Esto significa, por ejemplo, que un modelo de reconocimiento facial puede hacer determinaciones sobre las características de las personas en función de factores como la raza o el género sin que el programador se dé cuenta.
La tasa de aprendizaje también puede convertirse en un gran desafío para los modelos de aprendizaje profundo. Si la tasa es demasiado alta, entonces el modelo convergerá demasiado rápido, produciendo una solución menos que óptima. Si la tasa es demasiado baja, el proceso puede atascarse y será aún más difícil llegar a una solución.
Los requisitos de hardware para los modelos de aprendizaje profundo también pueden crear limitaciones. Se requieren unidades de procesamiento de gráficos (GPU) multinúcleo de alto rendimiento y otras unidades de procesamiento similares para garantizar una mayor eficiencia y un menor consumo de tiempo. Sin embargo, estas unidades son caras y consumen grandes cantidades de energía. Otros requisitos de hardware incluyen memoria de acceso aleatorio y una unidad de disco duro (HDD) o una unidad de estado sólido (SSD) basada en RAM.
Otras limitaciones y desafíos incluyen los siguientes:
El aprendizaje profundo es un subconjunto del aprendizaje automático que se diferencia por la forma en que resuelve problemas. El aprendizaje automático requiere un experto en el dominio para identificar la mayoría de las funciones aplicadas. Por otro lado, el aprendizaje profundo comprende las características de manera incremental, eliminando así la necesidad de experiencia en el dominio. Esto hace que los algoritmos de aprendizaje profundo tarden mucho más en entrenarse que los algoritmos de aprendizaje automático, que solo necesitan de unos segundos a unas pocas horas. Sin embargo, ocurre lo contrario durante las pruebas. Los algoritmos de aprendizaje profundo toman mucho menos tiempo para ejecutar pruebas que los algoritmos de aprendizaje automático, cuyo tiempo de prueba aumenta junto con el tamaño de los datos.
Además, el aprendizaje automático no requiere las mismas máquinas costosas y de gama alta y las GPU de alto rendimiento que el aprendizaje profundo.
Al final, muchos científicos de datos eligen el aprendizaje automático tradicional en lugar del aprendizaje profundo debido a su interpretabilidad superior o la capacidad de dar sentido a las soluciones. Los algoritmos de aprendizaje automático también se prefieren cuando los datos son pequeños.
Los casos en los que el aprendizaje profundo se vuelve preferible incluyen situaciones en las que hay una gran cantidad de datos, una falta de comprensión del dominio para la introspección de características o problemas complejos, como el reconocimiento de voz y la PNL.
Esta imagen ilustra el proceso de aprendizaje profundo.
Essas páginas simulam o layout de portais de grandes redes varejistas, plataformas de viagens, entre outras, atraindo usuários ...
Buscando estratégias para potencializar as vendas no período, empresas business-to-business apostam em campanhas de marketing e ...
Em um período de vendas tão estratégico como a Black Friday, sofrer uma interrupção operacional pode criar danos à imagem da ...
Todos los Derechos Reservados, 
			TechTarget, S.A de C.V 2013 - 2024 


Política de privacidad



Configuración de las Cookies



Configuración de las Cookies


